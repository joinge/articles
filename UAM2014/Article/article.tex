
%===============%                               %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
% DOCUMENTCLASS %                                See full option description in "mytemplate.cls"
%===============%                               %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
 
\documentclass[
   UAM                                          % Document type (non-standard)
 , 12pt                                         % Text size
%  , draft
%  , final                                        % Quality
%  , xelatex                                      % Use the XeLaTeX compiler
%  , biblatex                                     % Use 'biblatex' for references (best by far)
 , bibtex                                       % Use 'bibtex' for references (oldschool :)
%  , movie
%  , notodos                                      % Disable todos
 , layout
%  , defaultformat                                % Attempt to use default formatting options
%  , glossary                                     % Use a glossary
]{common/mytemplate}
% \bibliography{references}
\hypersetup{
   bookmarksopen=true
 , bookmarksopenlevel=2
}

\definecolor{tabBlue}{HTML}{AACCFF}
\newlength\oldparindent
\setlength\oldparindent{\parindent}

\newlength\figwidth
\setlength\figwidth{0.7\linewidth}

% \setlength\floatsep{\baselineskip}
\setcounter{topnumber}{1}
% \setlength\baselineskip{12pt}
% \selectfont

\renewcommand\vec[1]{\boldsymbol{#1}}
\newcommand\mat[1]{\boldsymbol{#1}}

\renewcommand*\P{\mat P}
\newcommand*\V{\mat V}
\newcommand*\M{\mat M}

% \usepackage{caption}
\usepackage{subcaption}
\usepackage{color}


% Tune floats
\renewcommand{\topfraction}{0.9}  % max fraction of floats at top
\renewcommand{\bottomfraction}{0.8} % max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
\setcounter{topnumber}{1}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     % 2 may work better
\setcounter{dbltopnumber}{2}    % for 2-column pages
\renewcommand{\dbltopfraction}{0.9} % fit big float above 2-col. text
\renewcommand{\textfraction}{0.07}  % allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.85}  % require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.85} % require fuller float pages


\begin{document}
% Load the references.bib file
% \bibliography{../../../library.bib}

\pagestyle{plain}
% \layout
% \printinunitsof{cm}
% \prntlen{\baselineskip}
% \the\textwidth
% \the\linewidth
% \prntlen{\textwidth}\\
% \prntlen\linewidth\\
% \prntlen\oddsidemargin\\
% \prntlen\oddsidemargin
% \newpage



\title{A GPU Sonar Simulator for Automatic Target Recognition}%
%
\author{Jo Inge Buskenes\firstAddress, Herman Midelfart\secondAddress, \O{}ivind Midtgaard\secondAddress}%
%
\begin{contact}
  \firstAddress Dept. of Informatics, P.O.Box 1080 Blindern, N-0316 OSLO\\
  \secondAddress Norwegian Defence Research Establishment (FFI) P.O.Box 25, NO-2027 Kjeller
\end{contact}%
%
\begin{contact}
Contact author: Jo Inge Buskenes\\
Dept. of Informatics, Univ. of Oslo, P.Box 1080 Blindern, N-0316 OSLO\\
\href{mailto:joibu@ifi.uio.no}{joibu@ifi.uio.no}
\end{contact}%
%
\begin{abstract}
Template matching is a common technique for automatic classification of objects in synthetic aperture sonar (SAS) images. The principle is to isolate an image segment containing the object of interest, correlate it with a set of template images, and assign it to the class of the template yielding the highest correlation coefficient. The challenge is to come up with a representative set of template images covering the actual configurations of object and seabed.

We target this challenge with a sonar simulator that first takes as input a seabed model derived from the real sonar image. Then it places a 3D object model on the seabed, renders the scene, and adds the resulting image to the template set. For every object type, position, alignment and material, the procedure is repeated, and a correlation coefficient computed. The best performance is obtained when these parameters are estimated from the sonar image as part of the classification process. The simulator is therefore written in OpenGL and OpenCL and runs on graphics processing units (GPUs). The result is a fast performing and portable on-the-fly template generator which can adapt to the characteristics of the current scene.
\end{abstract}%
%
\keywords{Sonar, SAS, simulator, template matching, OpenGL, OpenCL}

\section{Introduction}

Automatic target recognition (ATR) is an important component in autonomous underwater vehicles (AUVs), as it allows the vehicle to adapt its mission plan to e.g. revisit detected objects for closer examination. One way to classify these objects is by using template matching. The principle involved  is to isolate an image segment containing the object of interest, correlate it with a set of template images of the relevant object classes, and assign it to the class of the template yielding the highest correlation coefficient. The challenge is to come up with a representative set of template images covering the actual configurations of object and seabed. The alternative of using of a static, predetermined template library has a computational complexity that is exponential with the number of parameters. This limits its use to only a few parameters being coarsely sampled, ultimately yielding inaccurate results~\cite{Midelfart2010}.

We have developed a synthetic aperture sonar (SAS) simulator that avoids this problem by running sufficiently fast to create templates adapted to the actual scene as part of the classification process. This is achieved with the aid of the massive computing power in graphics processing units (GPUs) and optimized software libraries for scene rendering. The simulator loads a 3D model of the seafloor and an object class model into OpenGL, where emitted sound waves are modeled with a light source placed at the sonar transmit location. We assume rough, isotropic surfaces reflecting sound energy equally in all directions. This can be modeled with a Lambertian scattering model~\cite{Blake1993,Bell1995}, where the intensity of the backscattered sound only depends on the incidence angle of the transmitted signal onto the model surface. When rendering OpenGL is set up to produce an optical 2D intensity image and depth map that reveals the distance from each pixel to the propagation axis. For maximum flexibility this data is finally combined with OpenCL to produce the image templates.

Various simulators for high frequency, side-looking sonar imagery have been published, e.g.~\cite{Bell1997,Sammelm2003}. Most implementations have however prioritized accurate acoustic modeling at the expense of execution speed. One exception is the SIGMAS+ simulator in~\cite{Coiras2009a, Coiras2009b} which, similar to our approach, takes advantage of parallel processing at GPUs and uses OpenGL to render a 2D image from a 3D model, under the assumption of Lambertian scattering. However, SIGMAS+ is still aimed more towards realistically looking images, including effects like ambient noise, which is irrelevant for our purpose of template generation. Also, their simulator sums images obtained from multiple OpenGL rendering passes to create the sonar image, while we render once and post-process this result with OpenCL to tailor the sonar image with more flexibility.

% The ATR algorithms must, however, be sufficiently fast to process the sensor data onboard the AUV in delayed real-time.

% We have developed a simulator for synthetic aperture sonar (SAS) that creates object templates very quickly with the aid of the massive computing power in graphics processing units (GPUs). It loads a 3D model of the sea floor and an object of interest into OpenGL, where emitted sound waves are modeled with a light source placed at the sonar transmit location. We assume rough, isotropic surfaces that reflects sound energy equally in all directions. This can be modeled with a Lambertian scattering model~\cite{Bell1997} where the intensity of the backscattered sound is said to depend only on the on incidence angle of the transmitted signal onto the model surface. When rendering OpenGL is set up to produce an optical 2D intensity image and depth map that reveals the distance from each pixels to the propagation axis. For maximum flexibility this data is finally combined with OpenCL to produce the image templates.
% 
% By coincidence our implementation is not so different from the SIGMAS+ simulator developed at the NATO Undersea Research Centre (NURC)~\cite{Coiras2009a, Coiras2009b}. They both take advantage of OpenGL to render a 2D image from 3D models, are designed for side-looking sonar, and assume a Lambertian scattering model. However, while SIGMAS+ is geared more towards realistically looking sonar images, and include effects such as noise, we do not. Also, their simulator sums images obtained from multiple OpenGL rendering passes to create the sonar image, while we render once and post-process this result with OpenCL to tailor the sonar image full flexibility. 


\begin{figure}[t]\centering%
\graphicsAI[drawing,width=\linewidth]{gfx/specs.svg}%
\caption{Simulator specifications.}\label{specs}%
\end{figure}

\begin{figure}[t]\centering%
\graphicsAI[drawing,width=\linewidth]{gfx/simulator.svg}%
\caption{Simulator.}\label{buildup}%
\end{figure}

\section{Methods}

Creating a 2D view of an arbitrary complex 3D scene is a non-trivial matter. This is why we decided to use the core OpenGL pipeline to do this for us. OpenGL is a popular, well-matured and multi-platform application programming interface (API) for rendering 2D and 3D vector graphics. It relieves us from the intricacies of projecting vertices, faces and textures defined in a 3D space onto a suitable 2D image plane. This section explains how we set up OpenGL for this task, and then proceeds to describe how we post process the OpenGL images with OpenCL to form the final sonar image.


\subsection{Setting up the scene}

Before we can render anything with OpenGL we first have to set up the scene. This involves loading the models and then placing them where we want them with the proper orientation. 

Our models are stored in regular 3D model files. The simulator loads these with the Open Asset Import library (\texttt{assimp}). This is a portable open source library that supports loading a wide range of 3D model formats in a uniform manner. Given a model file it outputs the model as a node tree where node contains data such as vertices, facets and textures, all formatted in an OpenGL friendly way.

The next step is to scale, orient and position the objects in the scene, and finally project this scene onto an image plane. This is achieved by applying a set of transformations to each vertex in the model. In OpenGL, the set of transformations is usually formalized like this:
\begin{align*}
\bmat{x\\y\\0\\1}_\text{image} = \P \cdot \V \cdot \M \cdot \bmat{x\\y\\z\\1}_\text{model},
\end{align*}
where $\bmat{\cdot}_\text{model}$ is the loaded model, $\bmat{\cdot}_\text{image}$ is the output image, and $\M$, $\V$ and $\P$ are transformation matrices. The first transform we apply is the model matrix $\M$, which scales and rotates the model in its local coordinate system. Then we map this model into world coordinates using the view matrix $\V$. This involves translating the model to its respectful place in the world, and then moving it into the view of the camera. Finally the relevant part of the scene is projected onto the image plane with a projection matrix $\P$. 

\subsection{OpenGL rendering}

To produce the sonar templates we assume a rough, isotropic surface that reflects energy equally in all directions. This permits us to use a Lambertian scattering model where the backscatter intensity depends only on the incidence angle~\cite{Zhang1999}. It does not consider observation angle or sound frequency, but for the purpose of creating templates this is not needed.   

The rendered image will appear as if we placed a window at the sonar and looked through it in the direction of the image. This window is resized to make sure that the template image perfectly fills it.

\subsection{OpenCL post-processing}

The ``camera image'' rendered with OpenGL is not in along-track and cross-track coordinates as we want it to be. It does, however, tell us what parts of the scene that are visible from the sonar. OpenGL can also produce a depth map that reveals the distance to each of image pixels. This information can be converted to a ranged sonar-like image by simply adding up all the intensity values that share the same depth for each range line. We perform this computation in OpenCL, which allow general purpose programming on GPUs and can interoperate with OpenGL quite nicely. This way we keep all the calculations on the GPU.

% It is also possible to compute the sonar image using the OpenGL ``blend'' feature.


\section{Results \& Discussion}

\newlength\imgspacing\setlength\imgspacing{.5cm}
\setcounter{topnumber}{2}

\begin{figure}[p]\centering%
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/data_cylinder_submerged.svg}%
\caption{SAS image of a cylinder with 2.6\;m length and 0.53\;m radius.}\label{data_cylinder}%
\end{subfigure}
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/sim_cylinder_submerged_adaptive.svg}%
\caption{Cylinder simulation with parameters estimated from SAS image: Length 2.6\;m, burial depth 0.263\;m, aspect angle 105$^\circ$.}\label{sim_cylinder}%
\end{subfigure}
\caption{A SAS image of a cylinder and a template simulation adapted to it.}
\end{figure}

\newcommand\cdesc[2]{{\raggedright\setlength\fboxsep{0pt}
\fbox{\colorbox[HTML]{#1}{\vrule height8.5pt depth3.5pt width0pt\hspace{.5cm}}}\ \ #2\\}}
\begin{figure}[p]\centering%
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/overlay_cylinder_submerged.svg}%
\caption{Best template out of a predefined set. Inaccurate cylinder length, alignment and immersion depth. The segmented shadow and highlight regions are misaligned.}\label{overlay_cylinder}%
\end{subfigure}\hspace{\imgspacing}%
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[width=\linewidth]{gfx/overlay_cylinder_submerged_adaptive.svg}%
\caption{Adapted template. Template simulated with adapted parameters. Good fit in both highlight and shadow segments.}\label{overlay_cylinder_adaptive}%
\end{subfigure}
\hfill\parbox{\linewidth-0pt}{\small\centering
Segmentation overlay color description:\\[.5\baselineskip]
\begin{minipage}{.49\linewidth}
\cdesc{800000}{Template highlight and image highlight}
\cdesc{FF1000}{Template highlight only}
\cdesc{FFEB00}{Image highlight only}
\cdesc{83FF7C}{Background pixels}
\end{minipage}%
\begin{minipage}{.49\linewidth}
\cdesc{000083}{Template shadow and image shadow}
\cdesc{0014FF}{Template shadow only}
\cdesc{00EFFF}{Image shadow only}
\cdesc{00A7FF}{Template shadow and image highlight}
\end{minipage}}
\vspace{\baselineskip}
\caption{Segmented simulation image overlaid the segmented SAS image.}
\end{figure}

% Whenever conventional computer graphics processing can be used to 
% GPUs are designed to provide the best possible graphics processing performance, and there large ecosystem of tools and libraries  available 
% Using a GPU to drive a simulator is advantageous since
% 
% 
% Creating a GPU-based simulator is attractive since computer is an attractive solution as there are a  graphics traits. For one over a CPU-based standard simulator is two-fold: 

Our GPU-based simulator is very fast. On a computer with a six-core Intel Core i7-3930K and a Radeon HD7970 it computes almost 1000 templates per second, each being 1 megapixel large. This makes it possible to tailor the templates very closely imaged objects. In contrast, in the standard approach a template library must be created beforehand from a limited set of parameter values. Hence, it may happen that no template in the library is a close fit of the object in the image even though the object actually is a target of interest.

This performance was demonstrated on an image from the HISAS 1030 synthetic aperture sonar mounted on the HUGIN AUV. Out of the objects in the scene we selected a 2.6\;m long cylinder that was partly buried in the sea sediments (Fig. \ref{data_cylinder}). The length, immersion depth and aspect angle of this cylinder were estimated from the SAS image using our  adaptive template matching approach~\cite{Midelfart2010}.  Then these parameters were used by the simulator to create an adaptive template (Fig. \ref{sim_cylinder}), which is an almost perfect match of the cylinder. We also created standard templates for a regular template matching approach. In this case, we assumed that the cylinder mine had a generic length of 2\;m (like a MP80 or a Murena mine) and was proud on the seafloor. Moreover, templates were created for every 10$^\text{th}$ degree of the aspect angle. We believe these assumptions to be typical for a template library for cylinder mines. 

The standard and adaptive templates were then matched to the HISAS image. This is illustrated in Fig. \ref{overlay_cylinder} for a standard template and in Fig. \ref{overlay_cylinder_adaptive} for an adaptive template. These images were created by segmenting the image and the templates into highlight, shadow, and background regions. The regions from a template were then laid on top of the regions of the image to illustrate how well these regions matched. Note how the adaptive template obtained a much closer fit to image than the standard template. This was also reflected in the correlation scores (which were created with the method described in \cite{Midelfart2010}) that were 0.613 for the standard template and 0.813 for the adaptive template. Hence, the standard approach was less likely to classify the cylinder in the image correctly as the standard templates were not created specifically for the target.

\section{Conclusion}

One way to automatically classify objects in SAS images is to compare the imaged objects with a predefined set of templates. However, this is suboptimal as it is infeasible to create accurate templates for all the relevant configurations of seabeds and objects. To solve this we have implemented a SAS simulator that creates the templates in delayed real-time based on parameters estimated from the current scene. In our studies this improves the match between the SAS objects and their corresponding template significantly in most cases.

To obtain the delayed real-time performance we implemented the simulator on a GPU. These devices have a theoretical peak performance that is typically an order of magnitude higher than CPUs in a comparable price range. We found this potential to be effectively utilized with the well matured and highly optimized OpenGL graphics processing API. Our simulator use this framework for most of the scene processing.

A final post-processing step is performed in OpenCL, which allow general purpose GPU programming. It interoperates well with OpenGL and adds a lot of flexibility to the simulation process. This is valuable as the simulator is still being actively developed and will likely see new features that can not be easily implemented in OpenGL alone.

% 
% 
% \begin{itemize}
% \item Mer testing
% \item Robustifisering
% \item Tuning
% \end{itemize}
% 



% Ligger nesten med kortsiden til.
% Bilde og templat segmenteres hver for seg.
% Ikke adaptiv - darlig. Feil lengde, skyggen for kort (synket). Orientering feil. Feil spacing. 
% Adaptiv - malt objekt. Passer bra. 
% 
% Videre:
% - Mer testing
% - Robustifisering
% - Tuning

% 
\bibliography{references}
% \bibliography{../../../library.bib}
% 
% % If biblatex (file is loaded in preamble):
% % \printreferences

\end{document}
