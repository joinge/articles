
%===============%                               %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
% DOCUMENTCLASS %                                See full option description in "mytemplate.cls"
%===============%                               %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
 
\documentclass[
   UAM                                          % Document type (non-standard)
 , 12pt                                         % Text size
%  , draft
%  , final                                        % Quality
%  , xelatex                                      % Use the XeLaTeX compiler
%  , biblatex                                     % Use 'biblatex' for references (best by far)
 , bibtex                                       % Use 'bibtex' for references (oldschool :)
%  , movie
%  , notodos                                      % Disable todos
 , layout
%  , defaultformat                                % Attempt to use default formatting options
%  , glossary                                     % Use a glossary
]{common/mytemplate}
% \bibliography{references}
\hypersetup{
   bookmarksopen=true
 , bookmarksopenlevel=2
}

\definecolor{tabBlue}{HTML}{AACCFF}
\newlength\oldparindent
\setlength\oldparindent{\parindent}

\newlength\figwidth
\setlength\figwidth{0.7\linewidth}

% \setlength\floatsep{\baselineskip}
\setcounter{topnumber}{1}
% \setlength\baselineskip{12pt}
% \selectfont

\renewcommand\vec[1]{\boldsymbol{#1}}
\newcommand\mat[1]{\boldsymbol{#1}}

\renewcommand*\P{\mat P}
\newcommand*\V{\mat V}
\newcommand*\M{\mat M}

% \usepackage{caption}
\usepackage{subcaption}
\usepackage{color}


% Tune floats
\renewcommand{\topfraction}{0.9}  % max fraction of floats at top
\renewcommand{\bottomfraction}{0.8} % max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
\setcounter{topnumber}{1}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     % 2 may work better
\setcounter{dbltopnumber}{2}    % for 2-column pages
\renewcommand{\dbltopfraction}{0.9} % fit big float above 2-col. text
\renewcommand{\textfraction}{0.07}  % allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.85}  % require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.85} % require fuller float pages


\begin{document}
% Load the references.bib file
% \bibliography{../../../library.bib}

\pagestyle{plain}
% \layout
% \printinunitsof{cm}
% \prntlen{\baselineskip}
% \the\textwidth
% \the\linewidth
% \prntlen{\textwidth}\\
% \prntlen\linewidth\\
% \prntlen\oddsidemargin\\
% \prntlen\oddsidemargin
% \newpage



\title{A GPU Sonar Simulator for Automatic Target Recognition}%
%
\author{Jo Inge Buskenes\firstAddress, \O{}ivind Midtgaard\secondAddress, Herman Midelfart\secondAddress}%
%
\begin{contact}
  \firstAddress Dept. of Informatics, P.O.Box 1080 Blindern, N-0316 OSLO\\
  \secondAddress Norwegian Defence Research Establishment (FFI) P.O.Box 25, NO-2027 Kjeller
\end{contact}%
%
\begin{contact}
Contact author: Jo Inge Buskenes\\
Dept. of Informatics, Univ. of Oslo, P.Box 1080 Blindern, N-0316 OSLO\\
\href{mailto:joibu@ifi.uio.no}{joibu@ifi.uio.no}
\end{contact}%
%
\begin{abstract}
Template matching is a common technique used when classifying objects in synthetic aperture sonar (SAS) images. The principle is to isolate an image segment containing an object of interest, correlate it with a set of template images, and assign it to the class of the template yielding the highest correlation coefficient. The challenge is to come up with a suitable set of template images considering that no seabed or object is truly alike.

We target this challenge with a sonar simulator that first take as input a seabed model derived from the real sonar image. Then it places an object model on the seabed, renders the scene, and adds the resulting image to the template set. For any object position, alignment, type and material, the procedure is repeated, and a correlation coefficient computed. The faster we are able to perform these simulations, the better we can expect the classification result to be. Therefore the simulator is written in OpenGL and OpenCL and run on graphics processing units (GPUs).

The result is a fast performing, mobile and portable on-the-fly template generator which can adapt its behavior to the nature of the current scene. We believe this can prove a powerful tool for mobile sonar imaging platforms such as autonomous underwater vehicles (AUVs).
\end{abstract}%
%
\keywords{Sonar, SAS, simulator, template, OpenGL}

\section{Introduction}


% A synthetic aperture sonar (SAS) utilizes a wider along-track beam to ensonify the same part of the seafloor with several sucessive pings. Later this data is combined coherently to allow the synthetization of a much larger receiving array, which drastically improves the along-track resolution. 

% To obtain optimal classification performance using template matching techniques a large set of templates must be availab

We have developed a simulator for synthetic aperture sonar (SAS) that creates object templates very quickly with the aid of the massive computing power in graphics processing units (GPUs). It loads a 3D model of the sea floor and an object of interest into OpenGL, where emitted sound waves are modeled with a light source placed at the sonar transmit location. We assume rough, isotropic surfaces that reflects sound energy equally in all directions. This can be modeled with a Lambertian scattering model~\cite{Bell1997} where the intensity of the backscattered sound is said to depend only on the on incidence angle of the transmitted signal onto the model surface. When rendering OpenGL is set up to produce an optical 2D intensity image and depth map that reveals the distance from each pixels to the propagation axis. For maximum flexibility this data is finally combined with OpenCL to produce the image templates.

By coincidence our implementation is not so different from the SIGMAS+ simulator developed at the NATO Undersea Research Centre (NURC)~\cite{Coiras2009a, Coiras2009b}. They both take advantage of OpenGL to render a 2D image from 3D models, are designed for side-looking sonar, and assume a Lambertian scattering model. However, while SIGMAS+ is geared more towards realistically looking sonar images, and include effects such as noise, we do not. Also, their simulator sums images obtained from multiple OpenGL rendering passes to create the sonar image, while we render once and post-process this result with OpenCL to tailor the sonar image full flexibility. 


\begin{figure}[t]\centering%
\graphicsAI[drawing,width=\linewidth]{gfx/specs.svg}%
\caption{Simulator specifications.}\label{specs}%
\end{figure}

\begin{figure}[t]\centering%
\graphicsAI[drawing,width=\linewidth]{gfx/simulator.svg}%
\caption{Simulator.}\label{buildup}%
\end{figure}

\begin{itemize}
\item Midelfart/Midtgaard: Adaptive templates \cite{Midelfart2010}
\item Simulation of sidescan images (Bell thesis \cite{Bell1997}, pre-work by V.Blake \cite{Blake1993})
\end{itemize}
 

\section{Methods}

Creating a 2D view of an arbitrary complex 3D scene is a non-trivial matter. This is why we decided to use the core OpenGL pipeline to do this for us. OpenGL is a popular, well-matured, and multi-platform application programming interface (API) for rendering 2D and 3D vector graphics. It relieves us from the intricacies of handling vertices, faces and textures, as well as from projecting this onto a suitable 2D image plane. This section explains how we set up OpenGL to do this for us, and then proceeds to describe how we post process the OpenGL images with OpenCL to form the final sonar image.


\subsection{Setting up the scene}

Before we can render anything OpenGL we first have to set up the scene. This involves loading the models and then placing them where we want them with the proper orientation. 

Our models are stored in regular 3D model files. The simulator loads these with the Open Asset Import library (\texttt{assimp}). This is a portable open source library that supports loading a wide range of 3D model formats in a uniform manner. It handles the intricacies of the various model formats and outputs the model as a node tree. Each node contains data such as vertices, facets and textures, and since they are formatted in an OpenGL friendly way we simply iterate through the nodes and load it.

The next step is to scale, orient and position the objects in the scene, and finally project this scene onto an image plane. This is achieved by applying a set of transformations to each vertex in the model. In OpenGL, the set of transformations are usually formalized like this:
\begin{align*}
\bmat{x\\y\\0\\1}_\text{image} = \P \cdot \V \cdot \M \cdot \bmat{x\\y\\z\\1}_\text{model},
\end{align*}
where is the loaded model, $\bmat{\cdot}_\text{image}$ is the output image, and $\M$, $\V$ and $\P$ are transformation matrices. The first transform we apply is the model matrix $\M$, which scales and rotates the model in its local coordinate system. Then we map this model into world coordinates using the view matrix $\V$. This involves translating the model to its respectful place in the world, and then moving it into the view of the camera. Finally the relevant part of the scene is projected onto the image plane with a projection matrix $\P$. 

\subsection{OpenGL Rendering}

To produce the sonar templates we assume a rough, isotropic surface that reflects energy equally in all directions. This permits us to use a Lambertian scattering model where the backscatter intensity depends only on the incidence angle~\cite{Zhang1999}. It does not consider observation angle or sound frequency\todo{is this unproblematic?}.   

If we place the camera at the sonar and look in the direction of the image OpenGL will generate a image of all the pixels that are ``seen'' from the sonar. However, this can only provide us with optically looking images, while was not designed to does not produce camera where the sonar is and render the scene   handling computing the projection of a  and rather post-process its result with OpenCL to obtain 


When our models loaded into the OpenGL scene we are ready to render. 



It is also possible to compute the sonar image using the OpenGL ``blend'' feature.


\newpage

\section{Results \& Discussion}

\newlength\imgspacing\setlength\imgspacing{.5cm}

The main advantage of a GPU-based simulator over a standard simulator is its ability to create templates incredibly fast. This makes it possible to tailor the templates more closely to an image as the parameters needed by the simulator can be directly estimated from the image. In a standard approach, on the other hand, a template library must be created beforehand from a limited set of parameter values. Hence, it may happen that no template in the library is a close fit of the object in the image even though the object actually is a target of interest.

This performance was demonstrated on an image from the HISAS 1030 synthetic aperture sonar mounted on the HUGIN AUV. Out of the objects in the scene we selected a 2.6\;m long cylinder that was slightly buried in the sea sediments (Fig. \ref{data_cylinder}). The length, burial depth, and aspect angle of this cylinder were estimated from the SAS image using our  adaptive template matching approach~\cite{Midelfart2010}.  Then these parameters were used by the simulator to create an adaptive template (Fig. \ref{sim_cylinder}), which is an almost a perfect match of the cylinder. We also created standard templates for a regular template matching approach. In this case, we assumed that the cylinder mine had a length of 2\;m (like a MP80 or a Murena mine) and was proud on the seafloor. Moreover, templates were created for very 10$^\text{th}$ degree of the aspect angle. We believe the assumptions are quite typical for a template library for cylinder mines. 

The standard and adaptive templates were then matched to the HISAS image. This is illustrated in Fig. 4a for a standard template and in Fig. 4b for an adaptive template. These images were created by segmenting the image and the templates into highlight, shadow, and background regions. The regions from a template were then laid on top of the regions of the image to illustrate how well these regions matched. From these figures, it was quite clear the adaptive template obtained a much closer fit to image than the standard template. This was also reflected in the correlation scores (which were created with the method described in \cite{Midelfart2010}) that were 0.613 for the standard template and 0.813 for the adaptive template. Hence, the standard approach was less likely to classify the cylinder in the image correctly as the standard templates were not created specifically for the target.


\begin{figure}[tp]\centering%
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/data_cylinder_submerged.svg}%
\caption{SAS image of a cylinder with 2.6\;m length and 0.53\;m radius.}\label{data_cylinder}%
\end{subfigure}
\begin{subfigure}[t]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/sim_cylinder_submerged_adaptive.svg}%
\caption{Cylinder simulation with parameters adapted to fit the SAS image: Length 2.6\;m, burial depth 0.263\;m, aspect angle 105$^\circ$.}\label{sim_cylinder}%
\end{subfigure}
\caption{A SAS image of a cylinder and a template simulation adapted to it.}
\end{figure}

\setcounter{topnumber}{2}

\newcommand\cdesc[2]{{\raggedright\setlength\fboxsep{0pt}
\fbox{\colorbox[HTML]{#1}{\vrule height8.5pt depth3.5pt width0pt\hspace{.5cm}}}\ \ #2\\}}
\begin{figure}[tp]\centering%
\begin{subfigure}[b]{.5\linewidth-\imgspacing/2}
\graphicsAI[drawing,width=\linewidth]{gfx/overlay_cylinder_submerged.svg}%
\caption{Overlay.}\label{overlay_cylinder}%
\end{subfigure}\hspace{\imgspacing}%
\begin{subfigure}[b]{.5\linewidth-\imgspacing/2}
\graphicsAI[width=\linewidth]{gfx/overlay_cylinder_submerged_adaptive.svg}%
\caption{Overlay (adaptive).}\label{overlay_cylinder_adaptive}%
\end{subfigure}
\hfill\parbox{\linewidth-30pt}{
\cdesc{800000}{Pixels in both template and image highlight segments}
\cdesc{FF1000}{Pixels only in template highlight segment}
\cdesc{FFEB00}{Pixels only in image highlight segment}
\cdesc{83FF7C}{Background pixels}
\cdesc{00EFFF}{Pixels only in the image shadow segment}
\cdesc{00A7FF}{Pixels in the image segment, but in template shadow}
\cdesc{0014FF}{Pixels only in the image template highlight segment}
\cdesc{000083}{Pixels in both the template and image shadow segments}}
\end{figure}


\begin{table}[bh]
TODO: Probably not necessary to have this table...\\
\begin{tabular}[c]{l l l l l}\hline
\rowcolor{tabBlue} & cor   & ecor  & scor  & essum \\\hline
Standard template & 0.461 & 0.698 & 0.528 & 0.613 \\
Adaptive template & 0.673 & 0.818 & 0.807 & 0.813
\end{tabular}
\caption{Correlation scores. Description:\newline
cor - correlation between image and template calculated over all pixels in the image\newline
ecor - correlation calculated over the union of the echo pixels in the image\newline
scor - correlation calculated over the union of the echo pixels in image and template\newline
essum - 0.5*(ecor+scor). Average of the echo and shadows}
\end{table}

Reference correlation scores~\cite{Midelfart2010}

% Ligger nesten med kortsiden til.
% Bilde og templat segmenteres hver for seg.
% Ikke adaptiv - darlig. Feil lengde, skyggen for kort (synket). Orientering feil. Feil spacing. 
% Adaptiv - malt objekt. Passer bra. 
% 
% Videre:
% - Mer testing
% - Robustifisering
% - Tuning

% 
\newpage
\bibliography{references}
% \bibliography{../../../library.bib}
% 
% % If biblatex (file is loaded in preamble):
% % \printreferences

\end{document}
