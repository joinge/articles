%% !TeX root = IIII_simulator
% !TeX root = article

% Disallow floats on the first page
\suppressfloats

%Design, notation and application

\title{Real-Time Synthetic Aperture Sonar Simulation for Automatic Target Recognition: Notation and Application}

\author{{Jo~Inge~Buskenes, %
        Herman~Midelfart%, %
%        \O{}ivind~Midtgaard
        } %
\thanks{J. I. Buskenes is with both the Department of Informatics, University of Oslo, Norway, and the Norwegian Defense Research Establishment (FFI), Norway.}%
\thanks{H. Midelfart and \O. Midtgaard are with The Norwegian Defence Research Establishment (FFI), Norway.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised January 11, 2007.}
}

% The paper headers
\markboth{IEEE Journal of Oceanic Engineering}%
{A GPU Sonar Simulator for Automatic Target Recognition}


% Publishers ID mark:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEcompsoctitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
\todo{Revise abstract, and make a point of the notation system}Template matching is a common technique for automatic classification of objects in synthetic aperture sonar (SAS) images. The principle is to isolate an image segment containing the object of interest, correlate it with a set of template images, and assign it to the class of the template yielding the highest correlation coefficient. The challenge is to come up with a representative set of template images covering the relevant configurations of object and seabed.

We target this challenge with a sonar simulator that first takes as input a seabed model derived from the real sonar image. Then it places a 3D object model on the seabed, renders the scene, and adds the resulting image to the template set. For every object type, position, alignment and material, the procedure is repeated, and a correlation coefficient computed. The best performance is obtained when these parameters are estimated from the sonar image as part of the classification process. The simulator is therefore written in OpenGL and OpenCL and runs on graphics processing units (GPUs). The result is a fast performing and portable template generator which can adapt to the characteristics of the current scene on-the-fly.
\end{abstract}

% Keywords (normally not used for peer reviews)
\ifPeerReview\else
\begin{IEEEkeywords}
Sonar, SAS, simulator, template matching, OpenGL, OpenCL.
\end{IEEEkeywords}
\fi}
% \fi 

% make the title area
\maketitle

% This command fixes abstract positioning for compsoc articles:
\IEEEdisplaynotcompsoctitleabstractindextext

% (Optional) Add some extra info on cover page of peer review papers:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi

% Insert page break and insert second title (peer review mode)
\IEEEpeerreviewmaketitle



\section{Introduction}

\setcounter{totalnumber}{1}
\setcounter{topnumber}{1}
\setcounter{dbltopnumber}{1}

% \tikz[font=\scriptsize,overlay,baseline=(char.base)]{%
\providecommand\r*{}\renewcommand*\r[2]{%
\tikz[font=\scriptsize]{%
\node[inner sep=0pt](#1){$#2$};%
\begin{scope}[overlay]%
\coordinate(offsett) at (0.3em,1.2ex);%
\draw[-]($(#1.center) - (offsett)$) to ($(#1.center) + (offsett)$);%
\end{scope}%
}}
%\tracingmacros


\providecommand\m*{}\renewcommand*\m[2][\phantom{jf}]{\tikzmark{#2}{#1}}
\providecommand\k*{}\renewcommand*\k[2]{{\scriptsize\color{black}\tikzmark{#1}{$#2$}}}
\providecommand\g*{}\renewcommand*\g[2]{{\scriptsize\color{Green}\tikzmark{#1}{$#2$}}}
%\providecommand\r*{}\renewcommand*\r[2]{\scriptsize\color{red}\tikzmark{#1}{$#2$}}
\providecommand\b*{}\renewcommand*\b[2]{{\scriptsize\color{blue}\tikzmark{#1}{$#2$}}}

%\newcommand{\tikzmark}[2]{\tikz[remember picture,baseline=(#1.base)]\node[shape=rectangle,inner sep=0pt](#1){#1};}
% \draw[-{Latex[length=.5em,width=.5em]}]
%                         ($(C.center) + (offset) + (1.3em,-0.4ex)$) to[out=-30, in=90] ($(NR) + (offset) + (1.6em,0ex)$)
%       to[out=-90, in=0] ($(X.center) + (offset) + (0.8em,0ex)$);

\newlength\tpad\setlength\tpad{.2ex}
%\begin{tikzpicture}
%\coordinate(__) at ($ (0em,0ex) $);
%\end{tikzpicture}


% \newcommand\figstep[2]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt}{\textbf{\Green{#1}}}}}\mbox{}\hfill\parbox[t]{.97\linewidth}{#2}}

\begin{figure*}[t]\centering%
\ifOverLeaf%
  \includegraphics[width=\linewidth]{gfx/simulator.pdf}%
\else
  \includegraphics[drawing,width=\linewidth]{gfx/simulator.svg}%
\fi
\caption{\emph{Simulator concept}: Given as input 3D models of the object and seafloor, and some parameters for these, the simulator produces an image template, mask and a correlation coefficient. The \emph{conventional} (static) approach pick parameters from a predefined set, whereas the \emph{adaptive} approach estimate them from the sonar image. The implementation reside entirely on the GPU, with OpenGL performing scene rendering and OpenCL general-purpose post-processing. Compared to a CPU, the GPU's superior processing speed allow a much wider range of object parameters to be searched for an optimal template, yielding better results in a lesser time.}\label{IV_buildup}%
\end{figure*}

% \newcommand\figstep[2]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt}{\textbf{#1}}}}\mbox{}\hfill\parbox[t]{.97\linewidth}{#2}}
\begin{figure*}[t]\centering%
\ifOverLeaf%
  \includegraphics[width=\linewidth]{gfx/scene_coordinate_system_radial.pdf}%
\else
  \includegraphics[width=\linewidth]{gfx/scene_coordinate_system_radial.svg}%
\fi
\caption[caption]{\emph{Virtual scene geometry and processing steps.} As marked with encircled numbers in the figure, five steps are needed to obtain the sonar template:\\[.5\baselineskip]
%
\figstep{1}{\emph{Model displacement.} When 3D models of the seafloor and object are loaded into OpenGL, each has its own reference frame $\protect\ubar{M}$. The region we seek to image has a position and orientation $\ubar{I}$. We apply the displacement (translation and rotation) of $\ubar{M}$ relative to $\protect\ubar{I}$ with a \emph{model matrix} $\mat{T}_\textit{\tiny\!IM}$.
}\\[.5\baselineskip]
%
\figstep{2}{\emph{Sonar translation.} The sonar local body has the reference frame $\ubar{S}$. We set its orientation equal to that of the image, $\uvec{S}=\uvec{I}$, and apply the remaining translation of $\udot{I}$ relative to  $\udot{S}$ with a \emph{sonar matrix} $\mat{T}_\textit{\tiny\!SI}$.
}\\[.5\baselineskip]
%
\figstep{3}{\emph{Cartesian camera rotation.} To see the world as the sonar does, we position the OpenGL camera at the origin of the sonar frame, $\udot{C}=\udot{S}$, lock its horizontal axis to that of the sonar, $\hat{x}_C=\hat{x}_S$, and direct it straight at the origin of the image frame, $\hat{z}_C\parallel\vec{p}_{SI}$. The position is correct after step 2, which leaves applying the rotation of $\uvec{S}$ relative to $\uvec{C}$ with a \emph{Cartesian camera matrix} $\mat{T}_\textit{\tiny\!CS}$.
}\\[.5\baselineskip]
%
\figstep{4}{\emph{Radial camera coordinate conversion.} The $\hat{x}_C$ and $\hat{y}_C$ axes span an orthogonal viewing plane onto which the scene's points can be projected. However, to simplify the boundary specification of range and elevation in the next step, we convert the coordinate representation from Cartesian to radial (cylindrical) with a non-linear function T$_{RC}\colon (\hat{x}_C,\hat{y}_C,\hat{z}_C) \mapsto (\hat{x}_R,\hat{\phi}_R,\hat{r}_R)$ subject to $\hat{x}_R=\hat{x}_C$.
}\\[.5\baselineskip]
%
\figstep{5}{\emph{Normalization of coordinates}. OpenGL only renders coordinates that fall in the range $[-1,1]$. We map the coordinate boundaries (for along-track, range and elevation) into this normalized range with a \emph{normalization matrix} $\mat{T}_\textit{\tiny\!NR}$.
}\\[.5\baselineskip]
%
\figstep{6}{\emph{OpenGL rendering.} When OpenGL renders the scene, it uses a \emph{vertex shader} to apply our transformation matrices to every vertex and a \emph{fragment shader} to compute the intensity values for each point in the image. OpenGL computes the range from each image pixel to its corresponding model facet: this is called a \emph{depth map}.
}\\[.5\baselineskip]
%
\figstep{7}{\emph{OpenCL post-processing.} Finally, the sonar-like image is formed---for some unique azimuth coordinate---by accumulating all intensity values that share the same range. This is our template. %  along with the depth information is finally combined to form a sonar like image. This is our template.
}%
}\label{IV_simulator_coordinate_system}%
\end{figure*}

% \IEEEPARstart{W}{hen} 
% Autonomous underwater vehicles (AUVs). When an AUV with a sonar imaging system encounter an interesting object, it would be very beneficial if adapted its behavior to examine the object further. This is challenging as it would need to learn in what ways the interesting objects differ from the background, i.e. it relies on previous knowledge. It would also need to process this information is near real-time to act in an efficient manner.

% - Adapt simulation to its behaviour, see if they still match.

\IEEEPARstart{A}{utonomous} underwater vehicles (AUVs) are vital in the exploration of our underwater frontiers. When equipped with a modern synthetic aperture sonar (SAS), they map seafloors, survey underwater biology, help localize wrecks, inspect oil pipes, fields and electricity lines, or help remove mines, biohazard waste or plastic. However, they struggle to deal with unforeseen events, such as poor imaging conditions, blocked paths, corrupted data or rare objects. One reason is their lack of a reliable and adaptive system for automatic target recognition (ATR).

%However, Such independent systems have much to gain from being able to adapt to the scenario at hand. For instance, with a well designed automatic target recognition (ATR) system on board they can adapt their mission plan to collect more information on particularly interesting objects, or fine-tune the sonar and navigation system to create better images.
 
% Automatic target recognition (ATR) is an important component in autonomous underwater vehicles (AUVs), as it allows the vehicle to adapt its mission plan to e.g. revisit detected objects for closer examination. One way to classify these objects is by using template matching. The principle involved  is to isolate an image segment containing the object of interest, compare it with a set of template images of the relevant object classes, and assign it to the class of the template with the the best fit.

A conventional ATR system isolates an object of interest, extracts its key features, and uses these to estimate the object's class. The challenge is to find features that are unique and intrinsic, such as type, shape and size, independent of extrinsic ones such as alignment, burial depth, degeneration level or view angle. Such features are rare in SAS, despite its best-in-class resolution and dynamic range, due to image ambiguities from acoustical distortions such as layover, multiple scattering, diffraction, dispersion and penetration.


Template matching offers an alternative to feature extraction: to compare the object with image templates of relevant classes and orientations, and assign the object to the class that fits best. The problem is to find a set of templates that properly represent the objects. Too many templates are needed to densely sample each degree of freedom. Therefore, practical implementations resort to a few sparsely sampled features, ultimately yielding inaccurate results~\cite{Midelfart2010}.


Adaptive template matching is a hybrid solution: to extract the most reliable statistics from the image, such as shadow and highlight characteristics, then use these to limit the template solution space~\cite{Midelfart2010}. This enables a fine-grained search through more features, at a reduced computational load and improved accuracy. However, we can remove a template library completely by simulating the templates on-the-fly.

%, to adapt to much broader scenarios.


% estimates some To overcome the limitation on In previous work we propose to adaptive  implement adaptive template matching using partial feature extraction. 
%We implement a hybrid solution by selecting a template based on object and seafloor parameters estimated from the SAS image~\cite{Midelfart2010}.
%
%We address the efficiency and problems of conventional template matching with \emph{adaptive template matching}---selecting a template based on object and seafloor parameters estimated from the SAS image~\cite{Midelfart2010};\todo{   **template selection based on object and seafloor parameters from SAS image***       Takes up a bit too much space, considering it won't be described in great detail} and
%\emph{real-time template simulation}---creating a template tailored to the estimated parameters, to improve its accuracy and avoid the need of a precomputed template library. 

%
%\todo{However, only a few simulators for high frequency, side-looking sonar imagery have been published. ***}
A few simulators for high frequency, side-looking sonar imagery have been published, e.g.~\cite{Bell1997,Sammelm2003}. We seek a fast simulator for creating simple templates, but most implementations prioritize accurate acoustic modeling at the expense of execution speed. One exception is the SIGMAS+ simulator~\cite{Coiras2009a, Coiras2009b}, which is accelerated by the parallel computing power of graphics processing units (GPUs). Aligning with our goals, it assumes Lambertian scattering and creates the sonar image from a 3D model with multiple renderings by the OpenGL computer graphics library. However, it includes stochastic effects such as ambient noise, which we deem irrelevant for templates, and its OpenGL-centric nature makes it suboptimal as an adaptive template matcher.

%while we consider templates deterministic. Neither is

%is intended for computer graphics; a side-effect of OpenGL due to the limitations of OpenGL, which is intended for computer graphics.

%It also relies on multiple  rendering passes to create the sonar image\\

% Since OpenGL is primarily intended for computer graphics, This is a drawback of OpenGL, which leaves a lot to be desired in terms of flexibility and extensibility---OpenGL is not a general-purpose programming tool.\todo{Trim, too verbose}

% - lack of flexibility
% - lack of extensibility

% - 
% This enables fast and flexible general-purpose computing on the GPU.
% This allows tailoring the sonar image with greater flexibility and allows tighter integration with the classifier.\todo{Too vague} 
% on a new full geometrical implementation of the simulator.

Our simulator was designed from the ground up to be part of a GPU-accelerated adaptive template matcher. It uses both OpenGL and the general-purpose programming framework OpenCL; two libraries that can inter-operate on the GPU. We describe it using a new notation system for homogeneous coordinates that allows complicated geometries to be expressed in a rigorous, unified and simple\todo{Would you say ``in a simplified way?''} way. Its function is verified using data from Jesus Bay, Norway, recorded by the HISAS1030 100\,kHz SAS  mounted on a HUGIN AUV from Kongsberg Maritime.



\section{Methods}\label{IV_methods}


%
\begin{table*}[thbp]\centering
%
{
\newcommand\shiftup[1]{\raisebox{-.75\normalbaselineskip}{#1}}
\newcommand\shiftdown[1]{\smash{\raisebox{.75\normalbaselineskip}{#1}}}
\newcolumntype{L}{>{\collectcell\shiftdown}l<{\endcollectcell}}
\newcolumntype{C}{>{\collectcell\shiftdown}c<{\endcollectcell}}
\newcolumntype{O}{>{\collectcell\shiftdown\hspace*{-1.75\tabcolsep}}l<{\endcollectcell}}
\newcolumntype{Q}{>{\collectcell\shiftdown\hspace*{-1.75\tabcolsep}}c<{\endcollectcell}}
\newcolumntype{P}[1]{>{\collectcell\shiftdown}p{#1}<{\endcollectcell}}
\providecommand\t*{}\renewcommand*\t[1]{{\tikzmark{#1}{$\ubar{#1}$}}}
%	\hline\hspace*{-1.75\tabcolsep}
%\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%%
\def\arraystretch{1.5}
% \setlength{\belowrulesep}{0pt}
%\setlength{\extrarowheight}{0.5ex}
\newcommand\pt[1]{\parbox[c]{1.5em}{text}}
%\cellcolor[gray]{.9}
\newcommand\mc[3]{\multicolumn{#1}{#2}{#3}}
\providecommand*\T{}\renewcommand*\T[1]{\parbox[l]{2em}{$\vec{T}_{#1}$}}
%\providecommand*\Tr{}\renewcommand*\Tr[1]{\parbox[l]{3em}{$\dvec{T}_{#1}($}\parbox[l]{3em}{#2}\parbox[l]{3em}{#3}\parbox[l]{0.75em}{#4})}
\providecommand*\Tr{}\renewcommand*\Tr[1]{$\dvec{T}_{#1}$}
\providecommand*\inn{}\renewcommand*\inn[1]{}%\in\mathrm{#1}(3)}
\providecommand*\TT{}\renewcommand*\TT[1]{\parbox[l]{2em}{T$_{#1}$}}
\providecommand*\p{}\renewcommand*\p[2]{\parbox[l]{2em}{$\dvec{p}_{#1}^{#2}$}}
\providecommand*\SIM{}\renewcommand*\SIM{$\mathrm{Sim}(3)$}
\providecommand*\SE{}\renewcommand*\SE{$\mathrm{SE}(3)$}
\providecommand*\Emph{}\renewcommand*\Emph[1]{\shiftup{\emph{#1}}}
\newcommand{\REmph}[1]{\tikz[remember picture,baseline=(#1.base)]\node[transform canvas={yshift=.75\normalbaselineskip},align=center,rotate=30,inner sep=0pt](#1){#1};}
%\newcommand{\trot}[1]{\tikz[remember picture,baseline=(#1.base)]\node[shape=rectangle,inner sep=0pt](#1){#1};}
%\newcommand{\tikzmark}[2]{\tikz[remember picture,baseline=(#1.base)]\node[shape=rectangle,inner sep=0pt](#1){#1};}
% \node(ACe)[below = 2\baselineskip of CC, anchor=center]%
%    {\footnotesize\textrm{Change of vector}};
%\setlength{\tabcolsep}{1.5em} @{\extracolsep{0em}}
\newcommand\mcc[3]{\mc{#1}{#2}{\shiftup{#3}}}
%\providecommand*\REmph{}\renewcommand*\REmph[1]{{A}}%\raisebox{-1.5ex}{\rotatebox{25}{\emph{#1}}}}}
\newcommand\addtimes{\hspace*{1.92\tabcolsep}\makebox[0pt]{,}\hspace*{-1.92\tabcolsep}}
\newcommand\addtimess{\hspace*{\tabcolsep}\makebox[0pt]{$\times$}\hspace*{\tabcolsep}}
\newcommand\addin{\hspace*{\tabcolsep}\makebox[0pt]{$\in$}}
\newcommand\const[1]{%\cellcolor{black!64}
#1}
%

%(\cdot)\\[-.75\normalbaselineskip]\cmidrule(lr){3-20}\\[-0.75\normalbaselineskip]\rowcolor{tabBlue}-1.15em}
\begin{tabular}{r c >{\hspace*{-0.9em}} C >{\hspace*{0.5em}} O Q >{\hspace*{-.5em}} Q Q Q C O C C C L >{\hspace*{-2\tabcolsep}}L O O O >{\hspace*{\tabcolsep}} O O O >{\hspace*{\tabcolsep}}  O O @{\hspace*{\tabcolsep}\shiftdown{$\in$}\hspace*{\tabcolsep}} L}
	\topline\rowcolor{tabBlue}
   \mc{2}{c}{\bf Frame} & & \mc{5}{c}{\bf Transformation} & \mc{5}{c}{\bf Configuration space} & \mc{11}{c}{\bf Representation} \\
   \cmidwrap{\cmidrule(lr){1-2}\cmidrule(lr){3-8}\cmidrule(lr){9-13}\cmidrule(lr){14-24}}\rowcolor{tabBlue} 
	\emph{Description} & \emph{Symbol} & \Emph{} & \Emph{$\vec{T}$} & \mc{4}{O}{\Emph{$\colon$ Mapping}} & \Emph{$\mathcal{C}($} & \Emph{$\vec{p}$\addtimes} & \Emph{$\vec{R}$\addtimes} & \Emph{$s)$} &  \Emph{DoF$^{(1)}$} &  \mc{1}{l}{\emph{$\mat{T}$}} & \Emph{$($} & & \Emph{$\dvec{p}$} & \Emph{$\phantom{z},$}  & & \Emph{$\dvec{R}$} & \Emph{$\phantom{\phi},$} &  \Emph{$s$} & \mc{1}{>{\hspace{-1.75\tabcolsep}}l}{$)$} & \Emph{Group} \\%\Emph{Sum} \\
	\midline
	                                         Model & \t{M}             &&                                             &          &            &       &                           &  &                &           &                &   & \mcc{1}{L}{}      &                                                                      &      &      &      &         &           &         &     & \mcc{1}{C}{} &      \\
	                                         Image & \t{I}             &\figstep{1}{}& \T{IM}                                      & $\colon$ & $\ubar{M}$ & $\to$ & $\ubar{I}$                &  & $\mathbb{R}^3$ & $S^2 S$ & $\mathbb{R}^+$ & 7 & \Tr{IM}           & $($                                                                  & $x,$ & $y,$ & $z,$ & $\psi,$ & $\theta,$ & $\phi,$ & $s$ & $)$          & \SIM \\
	                                         Sonar & \t{S}             &\figstep{2}{}& \T{SI}                                      & $\colon$ & $\udot{I}$ & $\to$ & $\udot{S}$                &  & $\mathbb{R}^3$ &         &                & 3 & \Tr{SI}           & $($                                                                  & $x,$ & $y,$ & $z$ &  &  & &     & $)$          & \SE  \\
	                                         Cartesian camera & \t{C}             &\figstep{3}{}& \T{CS}                                      & $\colon$ & $\uvec{S}$ & $\overset{(2)}{\to}$ & $\uvec{C}$ &  &                & \phantom{$S$}     &                & 0 & \Tr{CS}           & $($                                                                  &      &      &      & $\psi$\makebox[0pt][l]{$^{(2)}$}  &           &         &     & $)$          & \SE  \\
                              
\cmidrule{14-24}\rowcolor{tabBlue}
   \mc{13}{L}{\cellcolor{white}\hspace{16.7em}\raisebox{-12pt}{\color{Green}\begin{tabular}{l l}\rowcolor{white}
   $\Uparrow$ & \emph{Affine manipulations} \\$\Downarrow$ & \emph{Coordinate manipulations}
   \end{tabular}}} & \mc{1}{l}{\emph{$\mat{T}$}}                 & \mc{9}{O}{\hspace*{0\tabcolsep}\Emph{$\colon$ Mapping}} &  \\\arrayrulecolor{Green}\hline\arrayrulecolor{black}%cline{10-20}
	                                               &                    \\[0\baselineskip]
	                                Radial camera & \t{R}             &\figstep{4}{}&                                             &                             & & &                 &  &                &           &                &   & \mc{11}{L}{\hspace*{-\tabcolsep}\begin{tabular}[c]{p{2.2em} @{ $\colon$} p{7em} @{ $\to$ } p{3.45em} @{ $\in$ } l} $\mathrm{T}_{RC}$ & $(x,y,z)   \in\mathbb{R}^3$          & $(x,\varphi,r)$ &  $\mathbb{R}S\mathbb{R}$ \end{tabular}} \\
	                                      Normalized coordinates & \t{N}             &\figstep{5}{}&                                             &                             & & &                 &  &                &           &                &   & \mc{11}{L}{\hspace*{-\tabcolsep}\begin{tabular}[c]{p{2.2em} @{ $\colon$} p{7em} @{ $\to$ } p{3.45em} @{ $\in$ } l} \Tr{NR}           & $(x,\phi,r)\in\mathbb{R}S\mathbb{R}$ & NDC$^{(3)}$  &  $[0,1]^3$ \end{tabular}} \\ \bottomrule
                                         %\end{tabular}\parbox[L]{8em}{} $\to$ \parbox[L]{3.8em}{NDC$^{(3)}$} \parbox[L]{1em}{$\in$} $[0,1]^3$}   \\ \bottomrule
\mc{10}{l}{$^{(1)}$Degrees of Freedom} \\
\mc{10}{l}{$^{(2)}$Subject to $\hat{x}_C = \hat{x}_S$ and $\hat{z}_C \parallel \vec{p}_{SI}$} \\
\mc{10}{l}{$^{(3)}$Normalized Device Coordinates}
%\mc{10}{l}{$^{(4)}$Subject to $\hat{x}_O = \hat{x}_S$ and $\hat{z}_O \parallel \vec{p}_{SI}$}
\end{tabular}
%	                                Cylindric view & \t{C}             &                                             &                                               &  &                &           &                &   & $\mathrm{T}_{RC}$ & \mc{8}{O}{\hspace*{0\tabcolsep}$\colon (x,y,z)\in\mathbb{R}^3$}&\mc{1}{O}{$\to (x,\phi,r)\in$}&$\mathbb{R}S\mathbb{R}$   \\
%	                                      Clipping & \t{X}             &                                             &                                               &  &                &           &                &   & \Tr{NR}           & \mc{8}{O}{\hspace*{0\tabcolsep}$\colon (x,\phi,r)\in\mathbb{R}S\mathbb{R}$}&\mc{2}{O}{\hspace*{-\tabcolsep}$\to [0,1]^3$}   \\ \bottomrule
%	                 \mc{10}{l}{$^{(1)}$Degrees of freedom} &  \\
%
%cartesian $\rightarrow$ cylindric
%orthographic projection
%\p{MV}{M}
%\p{IV}{I}
%\p{SV}{S}
%& \p{OV}{O}
%\p{CV}{C}
%\p{XV}{X}
\begin{tikzpicture}[overlay,color=Green,remember picture, shorten >=-3pt]
%
\tikzset{
  c/.style={
    anchor=south west,
    node font=\itshape,
%    draw,
    align=center,
    rotate=30,
    yshift=-.2ex,
    xshift=-0.5em,
    align=left
  }
}
%\node[c] at (Rot){\rotatebox{0}{Rotation}};
%\node[c] at (Tra){\rotatebox{0}{Translation}};
%\node[c] at (Sca){\rotatebox{0}{Scale}};
%\node[c] at (Sum){\rotatebox{0}{DoF}};
%\node[anchor=base,transform canvas={yshift=.75\normalbaselineskip},align=left,rotate=30,inner sep=0pt](Rot){Rotation};
%\draw[->] ($(M.center) - (1.3em,0ex)$) to[out=-90, in=90] ($(O.center) - (1.3em,-.7ex)$);
\coordinate(IM) at ($(I)!0.5!(M)$);
\coordinate(SI) at ($(S)!0.5!(I)$);
\coordinate(CS) at ($(C)!0.5!(S)$);
\coordinate(RC) at ($(R)!0.5!(C)$);
\coordinate(NR) at ($(N)!0.5!(R)$);
\coordinate(offset) at (0.5em,0ex); %5.3em
%\path let \p1 = (TL.west), \p2 = (RC) in coordinate (SW) at (\x1,\y2);
%\path let \p1 = (TL.east), \p2 = (RC) in coordinate (SE) at (\x1,\y2);
%\node[anchor=base,text width=\minWidth,align=\alignment,inner sep=0pt,inner xsep=\tabcolsep,outer sep=0pt] (n) {\strut$#1$}
%\node(affine)[left = 5.6em of IM, anchor=center]{\footnotesize\textrm{Affine}};
%\node(affine)[left = 5.6em of SI, anchor=center]{\footnotesize\textrm{Affine}};
%\node(affine)[left = 5.6em of CS, anchor=center]{\footnotesize\textrm{Affine}};
%\node(nonlin)[left = 5.6em of RC, anchor=center]{\footnotesize\textrm{Non-linear}};
%\node(project)[left = 5.6em of NR, anchor=center]{\footnotesize\textrm{Projective}};
% \node(step1)[anchor=center] at ($(X)+(2.8em,-0.2em)$){\figstep{1}{}};
%\draw[-]
%                        (FrameL.south) to[out=0, in=180] (FrameR.south);
\draw[-{Latex[length=.5em,width=.5em]}]
                        ($(R.center) + (offset) + (1.3em,-0.4ex)$) to[out=-30, in=90] ($(NR) + (offset) + (1.6em,0ex)$)
      to[out=-90, in=0] ($(N.center) + (offset) + (0.8em,0ex)$);
\draw[-{Latex[length=.5em,width=.5em]},draw,densely dotted]                
                        ($(C.center) + (offset) + (1.1em,-0.4ex)$) to[out=-45, in=90] ($(RC) + (offset) + (1.6em,0ex)$)
      to[out=-90, in=0] ($(R.center) + (offset) + (0.8em,0ex)$);
\draw[-{Latex[length=.5em,width=.5em]}]
                        ($(S.center) + (offset) + (1.3em,-0.4ex)$) to[out=-30, in=90] ($(CS) + (offset) + (1.6em,0ex)$)
      to[out=-90, in=0] ($(C.center) + (offset) + (0.8em,0ex)$);
\draw[-{Latex[length=.5em,width=.5em]}]
                        ($(I.center) + (offset) + (1.3em,-0.4ex)$) to[out=-30, in=90] ($(SI) + (offset) + (1.6em,0ex)$)
      to[out=-90, in=0] ($(S.center) + (offset) + (0.8em,0ex)$);
\draw[-{Latex[length=.5em,width=.5em]}]
                        ($(M.center) + (offset) + (0.44em,0ex)$) to[out=0, in=90] ($(IM) + (offset) + (1.6em,0ex)$)
      to[out=-90, in=0] ($(I.center) + (offset) + (0.8em,0ex)$);
%\draw[-{Latex[length=.5em,width=.5em]}]
%                        ($(M.center) + (offset) + (0.44em,0ex)$) to[out=0, in=90] ($(IM) + (offset) + (1.6em,0ex)$)
%      to[out=-90, in=0] ($(I.center) + (offset) + (0.44em,0ex)$) to[out=0, in=90] ($(SI) + (offset) + (1.6em,0ex)$);
%      to[out=-90, in=0] ($(S.center) + (offset) + (0.44em,0ex)$) to[out=0, in=90] ($(CS) + (offset) + (1.6em,0ex)$)
%      to[out=-90, in=0] ($(C.center) + (offset) + (0.8em,0ex)$);
%%\path[fill=gray,opacity=0.2](TL.north east)--(TL.north east)--(SE)--cycle;
%%%%\draw[->] ($(I.east) - (.7em,0ex)$) to[out=-180, in=180] ($(S.east) - (.7em,0ex)$);
%%%%\draw[->] ($(S.east) - (.7em,0ex)$) to[out=-180, in=180] ($(C.east) - (.7em,0ex)$);
%%%%\draw[->] ($(C.east) - (.7em,0ex)$) to[out=-180, in=180] ($(R.east) - (.7em,0ex)$);
%%%%\draw[->] ($(R.east) - (.7em,0ex)$) to[out=-180, in=180] ($(X.east) - (.7em,0ex)$);
%%%%\draw[->,yshift=2ex] (pic cs:M) -- (pic cs:I) ;
\end{tikzpicture}
}
\caption{Reference frames and their transformations. The upper half is an affine domain where any pair of frames are related by a displacement with a specific configuration and a chosen coordinate representation. The lower half is a domain where frames represent intermediary states between coordinate manipulations. All set-multiplications are cartesian.}\label{IV_tab_frames_transitions}
\end{table*}

\Fig{IV_buildup} illustrates the basic idea behind the simulator: to use OpenGL to render an intensity and depth image of an object and seafloor model, and OpenCL to convert these into an image template. We call the method conventional when its input parameters are predefined, and adaptive when they are estimated from the SAS image.


% Depending on whether the input parameters are predefined or estimated from the SAS image, the use the label conventional or adaptive approach, respectively.

\Fig{IV_simulator_coordinate_system} and \Tab{IV_tab_frames_transitions} detail the geometry and processing steps that will underpin most of the upcoming implementation details. The first four steps detail the transformations that a vertex $\udot{V}$ undergoes, from being represented in the reference frame of a model, $\ubar{M}$, to that of an OpenGL camera, $\ubar{C}$. The mathematical syntax comes from a rigorous navigational notation system and rule set developed by K.~Gade~\cite{Gade2018}, which has a free and decomposed form.
%be rthat of the image region  $\ubar{I}$ , a sonar $\ubar{S}$, an orthogonal and cylindric viewing plane, $\ubar{O}$ and $\ubar{C}$ respectively, and a clipping space $\ubar{N}$ (not shown in \Fig{IV_simulator_coordinate_system}).\todo{Not sure if about this sentence.}



The free form emphasizes geometrical reasoning, conceptual clarity and physical understanding, while the decomposed form ensures geometrical validity and clarity when constructing, manipulating and interpreting coordinates. This allows the designer to focus on higher level concepts rather than getting lost in implementation details.%, such as ensuring that a vectors rather than ensuring that the points are decomposed in the same frame).
\todo{Summarize rules. Example?}
%
% and ing,  coordinates; manipulating them can be tricky and tedious, their interpretation ambiguous, and their construction error prone. Strict discipline is needed to avoid constructing invalid geometrical operations, such as mixing coordinates from two different frames. The notation aids system unambiguously expresses geometrical meaning and validity, thus allowing the designer to focus on higher level concepts rather than getting lost in implementation details. 

We adhere Gade's notation for affine geometry, extend it to handle homogeneous coordinates and projective geometry, and even use it to express non-linear effects. This facilitates transitioning between frames in a unified and geometrically valid way.




\subsection{Free notation}\label{IV_sec:general_notation}


Suppose a stationary $N$-dimensional rigid model $\mathcal{M}$ has a reference frame $\ubar{M} \triangleq (\udot{M},\uvec{M})$ attached to it, where $\udot{M}\in\mathbb{A}$ is its absolute position and $\uvec{M}\triangleq\big(\hat{x}_{M},\hat{y}_{M},\dots\big)\in\mathbb{V}^N$ its absolute orientation (where $\hat{\cdot}$ denotes unit length)\todo{Line above not close enough?}.
%
%
%
%The set of all possible configurations for the model is called its C-space, its dimension matching the number of degrees of freedom and its shape matching the topology. Rigid bodies have $N(N+1)/2$ degrees of freedom, $N$ Euclidean ones for position (topology $\mathbb{E}^N$), and $N(N-1)/2$ angular ones for orientation (topology $S^2S^1$ or $S^1$ for $N=\{3,2\}$, respectively).
%
%
A vertex $\udot{V}$ in the model can be related to the frame by a position vector $\vec{p}_{MV}\in\mathbb{V}$ going from $\udot{M}$ to $\udot{V}$,
%
\begin{align}
\vec{p}_{MV} &\triangleq \udot{V} - \udot{M},\label{eq_position_vector}
\end{align}
%
where $\vec{p}_{MV} = -\vec{p}_{V\!M}$\todo{Only way to change this formatting is to have more text before the next paragraph}. 

An image frame $\ubar{I}$ specifies the location and orientation of the seafloor area where we want to place the model. We express the displacement $\vec{T}_{IM}$ of $\ubar{M}$ relative to $\ubar{I}$,
%
\begin{align}
\vec{T}_{IM} \colon \ubar{M} &\to \ubar{I},\label{eq_rotation_dyadic}
\intertext{in terms of a translation $\vec{p}_{IM}$ that relates their positions,}
\vec{p}_{IM} \triangleq \udot{M} &- \udot{I},\label{eq_position_vector}
\intertext{and a rotation $\vec{R}_{IM}$ that relates their orientations,}
\vec{R}_{IM} \colon \uvec{M} &\to \uvec{I}\label{eq_rotation_dyadic}\\\nonumber
\vec{m} &\mapsto \vec{i}.
\end{align}
%
We can write the configuration of $\vec{T}_{IM}$ as $\mathcal{C}_{IM}=(\vec{p}_{IM},\vec{R}_{IM})$.
%
%%$\vec{p}_{IM}$ has $N$ degrees of freedom and Euclidean topology $\mathbb{R}^N$. $\vec{R}_{IM}$
%
%%It has $N(N+1)/2$ degrees of freedom: $N$ for the translation, and the rest for orientation. 
%The translation has $N$ degrees of freedom and topology $\mathbb{R}^N$. The rotation has $N(N-1)/2$ degrees of freedom with topology $\{S^2\times{}S^1, S^1\}$ for $N=\{3,2\}$.
%
It has $N(N+1)/2$ degrees of freedom: $N$ for the translation, with Euclidean topology $\mathbb{R}^N$; and $N(N-1)/2$ for rotation, with angular topology $\{S^2\times{}S^1, S^1\}$ for $N=\{3,2\}$. The symbol $\times$, which denotes the cartesian product of sets, will be dropped and assumed implicit in set-multiplications from now on. All frames and their configurations are listed in \Tab{IV_tab_frames_transitions}, along with an associated numeric representation. We describe this next.



\subsection{Decomposed notation}\label{IV_decomposed_notation}

Suppose we reference the model's vertices from the image frame $\ubar{I}$,
%
\begin{align}\label{IV_eq_free_vector_addition}
\m{CL}
\vec{p}_{\g{IV}{IV}}
= \vec{p}_{\g{I}{I}\r{M1}{M}}
+ \vec{p}_{\r{M2}{M}\g{V}{V}}.
\m{CR}
\\\nonumber
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
  \coordinate(_MV) at ($    (V.south)  + (-1.0\baselineskip,-1.0\baselineskip) $);
  \coordinate(_IV) at ($    (IV.south) + ( 1.0\baselineskip,-1.0\baselineskip) $);
  \coordinate(_IM) at ($    (I.south)  + (-1.0\baselineskip,-1.0\baselineskip) $);
  \coordinate(_IV_IM) at ($ .5*(_IV) + .5*(_IM) $);
  % Draw arrows and text
    \draw[]                    ($ (V.south)  - (0em,\tpad) $)
            to[out=-90, in=0]     (_MV)
            to[out=180, in=0]     (_IV_IM)
            to[out=180, in=-90]($ (IV.south) - (0em,\tpad) $);
    \draw[-]                   ($ (I.south)  - (0em,\tpad) $)
            to[out=-90, in=0]     (_IV_IM);
    }
\end{align}
%
where $\r{M}{M}$ is a \todo{end sentence}.
This vector addition (and any other vector-vector operation) is numerically possible and valid if---and only if---all vectors are decomposed, or represented, in the same frame. Since position vectors have Euclidean topology, we represent $\vec{p}$ in $\ubar{I}$ with a real number for each degree of freedom, $\dvec{p}^I\in\mathbb{R}^N$, and unambiguously rewrite \eq{IV_eq_free_vector_addition} in decomposed form as
%
\begin{align}
\dvec{p}_{IV}^{\Green{I}} &= \dvec{p}_{I\r{M1}{M}}^{\Green{I}} + \dvec{p}_{\r{M2}{M}V}^{\Green{I}}.\label{eq_head_tail_decomposed}
\end{align}
%
% Inverse $\dvec{R}^{-1} = \dvec{R}^T$\\
% Closure $\dvec{R}_1\dvec{R}_2 \in\mathrm{SO(3)}$\\
% Associative but not commutative
% 
% 
To obtain $\dvec{p}_{MV}^I$ we must rotate $\dvec{p}_{MV}^M$ by $\vec{R}_{IM}$. Since rotations have angular topology, we represent them in a space with more dimensions than the number of degrees of freedom, which avoids issues with rapidly changing values and singularities (e.g. Gimbal lock). A common choice is a rotation matrix $\dvec{R}\in\mathrm{SO}(N)$, i.e. a special (right handed) orthogonal $\mathbb{R}^{N\times{}N}$ matrix, with the properties $|\dvec{R}|=1$ (left-handed would be $-1$) and $\dvec{R}^T\dvec{R} = \dvec{I}$. The latter property constrains the column vectors of $\dvec{R}$ to be independent (orthogonal) and of unit length. This leaves $\dvec{R}$ with $N(N-1)/2$ degrees of freedom, as desired. Thus, the representation of $\vec{R}_{IM}$ in $\uvec{I}$ is $\dvec{R}_{IM}^I = \dvec{R}_{IM}\in \mathrm{SO}(N)$, subject to $\uvec{M} \mapsto \uvec{I}$,
%
\begin{align}\nonumber\\
\m{CL}
\dvec{p}_{\k{MV1}{MV}}^{\g{I1}{I}}
= \mat{R}_{\g{I2}{I}\r{M1}{M}} \; \dvec{p}_{\k{MV2}{MV}}^{\r{M2}{M}}.\m{CR}
\label{eq_passive_rotation}
%
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
  \coordinate(R) at ($ .5*(CL.north)  + .5*(CR.north) + (0em,1\baselineskip) $);
  % Compute arrow via points
  \path let \p1 = ($ (I1) + (1.0\baselineskip,0em) $),  \p2 = (R) in coordinate (_I1)  at (\x1,\y2);
  \path let \p1 = ($ (I2) - (1.0\baselineskip,0em) $),  \p2 = (R) in coordinate (_I2)  at (\x1,\y2);
  \path let \p1 = ($.5*(M1) +.5*(M2)$),  \p2 = (R) in coordinate (_M)  at (\x1,\y2);
  % Draw arrows and text
%    \draw[]                  ($ (M2.north) + (0em,\tpad) $)
%           to[out= 90, in=0]    (_M)
%           to[out=180, in=90]($ (M1.north) + (0em,\tpad) $);
    \draw[]                  ($ (I2.north) + (0em,\tpad) $)
           to[out= 90, in=0]    (_I2)
           to[out=180, in=0]    (_I1)
           to[out=180, in=90]($ (I1.north) + (0em,\tpad) $);
}
\end{align}
%
When $\dvec{R}_{IM}$ is left-multiplied as here, it should be interpreted as the \emph{orientation} of $\uvec{M}$ \emph{represented} in $\uvec{I}$, and \emph{vice versa} if it is right-multiplied. The rotation is associative, generally non-commutative and closed under multiplication.  %Embee
%, and the orthogonality follows from the skew symmetry of the axis of rotation. %SO$(N)$ are also known as rigid transformations, because they preserve the distance and orientation between points~\cite{Murray1994}\footnote{Pages 26-27}\todo{Valid page specification?}.
%
Note that $\dvec{R}_{IM} = \dvec{R}_{IM}^I = \dvec{R}_{IM}^M$ because $\dvec{p}_{MV}^M$ is rotated about a $\mathbb{R}^{N-2}$-subspace that is fixed in both $\ubar{I}$ and $\ubar{M}$. For the two- and three-dimensional cases the rotations are about\todo{around?} fixed points and lines, respectively.

The rotation in \eq{eq_passive_rotation} is called passive because its geometric intent is to change the representation but leave the absolute position of the point unchanged. An active transformation, by contrast, changes the vector but not the representation. A notational remark on this can be found in Appendix \ref{IV_active_rotations_transforms}, but only passive transformations will be needed henceforth.


\subsection{Homogeneous transformations}

Any transformation in $\mathbb{R}^N$-space can be represented by a transformation matrix $\dvec{T} \in \mathbb{R}^{N+1,N+1}$. In the affine case, it has the form of a Special Euclidean matrix $\dvec{T}\in\mathrm{SE}(N)$,
%
\begin{align}
\dvec{T} &= 
\left[\begin{array}{c c}
 \mat{R}  & \dvec{p} \\%\hline
 \dvec{0}^T  &  1
\end{array}\right] = \mathrm{SE}(N)\label{eq_homogeneous_affine_transform}
\end{align}
%
where both $\mat{T}$, $\dvec{p}\in\mathbb{R}^N$ and $\mat{R}\in\mathrm{SO}(N)$ are decomposed in the same frame, and $\dvec{0}^T$ is a row zero-vector of length $N$. If the last row is altered in any way, the transformation matrix becomes real projective, $\mat{T}\in\mathbb{RP}(N)$. Analogous to rotation matrices it is associative, generally non-commutative and closed under multiplication. %The regular affine version  Steps \step{1} to \step{4} 

As we dive into the specifics of each transformation, or step, look to \Tab{IV_tab_frames_transitions} for details on its configuration and coordinate representation, to \Fig{IV_simulator_coordinate_system} for visual aid, and to the supplementary Python script for implementation details. Assume the world as three dimensional, and note that each step has a label in the figure, a line in the table, and a matching heading number in the upcoming text.


%  In regular world space we use the affine version in \eq{eq_homogeneous_affine_transform}, but the last step is performing non-linear effects \Tab{IV_tab_frames_transitions} \eq{eq_homogeneous_affine_transform}

%There is nothing special with this matrix, it simply combines a rotation and translation into a single operation. If the latter row 
%
%The vertex $\udot{V}$ is now referenced from $\ubar{I}$, but we prefer it referenced from the position of the sonar local body $\udot{S}$, with an orientation $\uvec{O}$ that creates a viewing plane orthogonal to $\vec{p}_{SI}$. This mimics a camera placed at the acoustic source directed at the part of the seafloor we seek to image. For this we seek the These transforms take the form of \eq{eq_homogeneous_transformation} and can be stacked together,
%%
%\begin{align}
%\left[\begin{array}{c}
%\\\dvec{p}_{OV}^{O}\\\\\hline 1
%\end{array}\right]
%= \mat{T}_{CS}\;\mat{T}_{SI}\;\mat{T}_{IM}
%\left[\begin{array}{c}
%\\\dvec{p}_{MV}^{M}\\\\\hline 1
%\end{array}\right]
%\end{align}


%\figstep{1}{\emph{Model displacement.} When the 3D models of seafloor and object are loaded into OpenGL they each have their own reference frame $\protect\ubar{M}$. The region we seek to image has a position and orientation $\ubar{I}$. We apply the displacement (translation and rotation) of $\ubar{M}$ relative to $\protect\ubar{I}$ with a \emph{model matrix} $\mat{T}_\textit{\tiny\!IM}$.
%}\\[.5\baselineskip]
%
%When the 3D models of seafloor and object are loaded into OpenGL they each have with their own reference frame $\ubar{M}$. To position and orient the models into the image frame $\protect\ubar{I}$, we apply the \emph{model matrix} $\mat{T}_\textit{\tiny\!IM}$.

%%
%\figstep{2}{\emph{Sonar translation.} The sonar local body has the reference frame $\ubar{S}$. Its orientation is equal to that of the image, $\uvec{S}=\uvec{I}$. We apply the remaining translation of $\udot{I}$ relative to  $\udot{S}$ with a \emph{sonar matrix} $\mat{T}_\textit{\tiny\!SI}$.
%}\\[.5\baselineskip]
%%
%\figstep{3}{\emph{Orthogonal view rotation.} We want to position the OpenGL camera at the origin of the sonar frame, $\udot{S}$, looking at (i.e. $\hat{z}_O$-axis pointing at) the origin of the image frame, $\udot{I}$. While the  position is correct after step 2, we must apply the rotation of $\uvec{S}$ relative to $\uvec{O}$ with an \emph{orthogonal view matrix} $\mat{T}_\textit{\tiny\!SI}$.
%}\\[.5\baselineskip]
%%
%\figstep{4}{\emph{Coordinate conversion.} The $\hat{x}_O$ and $\hat{y}_O$ axes span an orthogonal viewing plane onto which the scene's points can be projected. However, to simplify the boundary specification of range and elevation in the next step, we convert the coordinate representation from cartesian $(\hat{y}_O,\hat{z}_O)$ to cylindrical $(\hat{\phi}_C,\hat{r}_C)$ with a non-linear function T$_{RC}$.
%}\\[.5\baselineskip]
%%
%\figstep{5}{\emph{Projection}. OpenGL only renders coordinates that fall in the range $[-1,1]$. We map the coordinate boundaries (for along-track, range and elevation) into this normalized range with a \emph{projection matrix} $\mat{T}_\textit{\tiny\!NR}$.
%% 
%% The size of this plane is determined by the desired image region on the seafloor. The projection is performed with the \emph{projection matrix} $\mat{T}_\textit{\tiny\!C}$.
%}\\[.5\baselineskip]
%%
%\figstep{6}{\emph{OpenGL rendering.} When OpenGL renders the scene it uses a \emph{vertex shader} to apply our transformation matrices to every vertex, and a \emph{fragment shader} to compute the intensity values for each point in the image. OpenGL computes the range from each image pixel to its corresponding model facet: This is called a \emph{depth map}.
%}\\[.5\baselineskip]
%%
%\figstep{7}{\emph{OpenCL post-processing.} Finally, the sonar-like image is formed---for some unique azimuth coordinate---by accumulating all intensity values that share the same range. This is our template. %  along with the depth information is finally combined to form a sonar like image. This is our template.
%}%



\subsubsection{Model displacement $\dvec{T}_{\!IM}$}

We previously found that the representation of $\mathcal{C}_{IM}$ in $\uvec{I}$ can be written $\boldsymbol{\mathcal{C}}_{IM}^I = (\dvec{p}_{IM}^I,\mat{R}_{IM})$. To allow resizing the model, we now also add a scaling factor $s_{IM}$ (see \Tab{IV_tab_frames_transitions}). By inserting $\boldsymbol{\mathcal{C}}_{IM}^I$ into \eq{eq_homogeneous_affine_transform} we obtain the transformation $\vec{T}_{IM}\colon \ubar{M}\to\ubar{I}$ represented in $\ubar{I}$,
%
\begin{align}\nonumber\\
\dvec{T}_{\g{IM1}{IM}}^{\k{I1}{I}} &= 
\left[\begin{array}{c c}
 \k{C}{}\mat{R}_{\g{IM2}{IM}}  & \dvec{p}_{\g{IM3}{IM}}^{\k{I2}{I}} \\%\hline
 \dvec{0}^T  &  s_{\g{IM4}{IM}}^{-1}
\end{array}\right] \nn &\in
\begin{cases}
\mathrm{SE}(3) & \mathrm{when}\ s_{IM}=1,\\
\mathrm{Sim}(3) & \mathrm{otherwise},
\end{cases} \label{eq_T_IM}
%
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
  \coordinate(R) at ($ (I2) + (0em,1\baselineskip) $);
  % Compute arrow via points
  \path let \p1 = ($ (I1) + (1.0\baselineskip,0em) $),  \p2 = (R) in coordinate (_I1)  at (\x1,\y2);
  \path let \p1 = ($ (I2) - (1.0\baselineskip,0em) $),  \p2 = (R) in coordinate (_I2)  at (\x1,\y2);
  % Draw arrows
    \draw[]                  ($ (I2.north) + (0em,\tpad) $)
           to[out= 90, in=0]    (_I2)
           to[out=180, in=0]    (_I1)
           to[out=180, in=90]($ (I1.north) + (0em,\tpad) $);
}
\end{align}
%
where Sim(3) is the set of $\mathbb{R}^{4\times4}$ similarity matrices that scales objects but otherwise preserves them (like the SE(3) group). Note that $\mat{T}_{IM}^I \ne \mat{T}_{IM}^M$ whenever a translation is involved, so the representation must usually\todo{lol you wanted usually instead of generally here...} be specified to avoid ambiguity.

The transformation matrix usage is similar to that of the rotation matrix, except it requires a homogeneous coordinate $[w\dvec{p}_{D\cdot}^E;w]\T\in\mathbb{P}$ as input, where $w\in\mathbb{R}$ (typically 1),
%
\begin{align}\nonumber\\
\dvec{p}_{\g{IV}{IV}}^\k{I1}{I} &= \mat{T}_{\g{I2}{I}\r{M1}{M}}^{\k{I3}{I}}\; 
\left[\begin{array}{c}\dvec{p}_{\r{M2}{M}\g{V1}{V}}^{\r{M3}{M}} \\ \tikzmark{One}{1}
\end{array}\right].
\\\nonumber
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
%  \coordinate(_MV)  at ($   (V.south)    + (-1.0\baselineskip,-1.0\baselineskip) $);
  \path let \p1 = ($ (V1.south) + (-1.0\baselineskip,0ex) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_One)  at (\x1,\y2);
  \path let \p1 = ($ (IV.south) + ( 1.0\baselineskip,0ex) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_IV)  at (\x1,\y2);
  \path let \p1 = ($ .5*(_IV) + .5*(_One) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_IV_V)  at (\x1,\y2);
  \path let \p1 = ($ .5*(_IV) + .5*(_One) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_IV_V)  at (\x1,\y2);
  \path let \p1 = ($ (I2.south) + (-1.0\baselineskip,0ex) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_I2)  at (\x1,\y2);
%  \path let \p1 = ($ (I1.north) + (1.0\baselineskip,1.0\baselineskip) $), \p2 = ($ (One.south) + (0em,-1.0\baselineskip) $) in coordinate (_I2)  at (\x1,\y2);
  \coordinate(_I1)  at ($ (I1.north) + ( 1.0\baselineskip,1.0\baselineskip) $);
  \coordinate(_I3)  at ($ (I3.north) + (-1.0\baselineskip,1.0\baselineskip) $);
  \coordinate(_I13)  at ($ .5*(_I1) + .5*(_I3) $);
%  \coordinate(_IV)  at ($   (IV.south)   + ( 1.0\baselineskip,-1.0\baselineskip) $);
%  \coordinate(_One) at ($   (One.south)  + (-1.0\baselineskip,-1.0\baselineskip) $);
%  \coordinate(_IV_V) at ($ .5*(_IV) + .5*(_One) $);
  % Draw arrows and text
%           to[out=180, in=0]    (_I1)
    \draw[]                  ($ (I3.north) + (0em,\tpad) $)
           to[out= 90, in=0]    (_I13)
           to[out=180, in=90]($ (I1.north) + (0em,\tpad) $);
    \draw[]                    ($ (V1.south)  - (0em,\tpad) $)
            to[out=-90, in=0]     (_One)
            to[out=180, in=0]     (_IV)
            to[out=180, in=-90]($ (IV.south) - (0em,\tpad) $);
    \draw[-]                   ($ (I2.south) - (0em,\tpad) $)
            to[out=-90, in=0]     (_I2);
    }
\end{align}
%
When $\mat{T}_{IM}^I$ is left-multiplied as here, interpret it as the \emph{displacement} of $\ubar{M}$ \emph{represented} in $\ubar{I}$, and \emph{vice versa} if it is right-multiplied. This is analogous to the rotation matrix. 

We parametrize the translation in $\dvec{T}_{IM}^I$ by three real numbers,
%
\begin{align}
\dvec{p}_{IM}^I &= [x,y,z]\T \in \mathbb{R}^3,
\end{align}
%
the rotation by the Euler angles yaw, pitch and roll $(\psi,\theta,\phi) \in [-\pi,\pi]^3$,
%
\begin{align}
\mat{R}_{IM}    &= \mat{R}_{\mathrm{Euler}}(\psi,\theta,\phi) \in SO(3),
\end{align}
%
and the scaling factor by a positive number that shrinks the model if it is less than 1, and \emph{vice versa} if it is greater. For a definition of $\mat{R}_{\mathrm{Euler}}$, see Appendix \ref{IV_transformation_matrices}.


\subsubsection{Sonar translation $\dvec{T}_{\!SI}$}

The sonar observes the world some distance from the image region, $\udot{S}\ne\udot{I}$, but we let it share its orientation, $\uvec{S}=\uvec{I}$. This reduces the transformation to a translation $\vec{T}_{SI}\colon \udot{I}\to\udot{S}$, which we represent in $\ubar{S}$ as
%
\begin{align}
\dvec{T}_{SI}^S &= 
\left[\begin{array}{c c}
 \mat{I}  & \dvec{p}_{SI}^S \\%\hline
 \dvec{0}^T  &  1
\end{array}\right] \in \mathrm{SE}(3), \label{eq_T_SI}
\end{align}
%
where $\mat{I}\in\mathbb{R}^3$ is the identity matrix and 
%
\begin{align}
\dvec{p}_{SI}^S &= [x,y,z]\T \in \mathbb{R}^3.
\end{align}
%


\subsubsection{Cartesian camera rotation $\dvec{T}_{\!CS}$}

%To perceive the world as a sonar would, we need to place our OpenGL camera at the sonar's position.

An OpenGL camera has the $x$-axis pointing right, the $y$-axis pointing up and the $z$-axis pointing into the scene. Since this is a left-handed coordinate system---and we prefer right-handed---we define our camera $\ubar{C}$ with the $x$-axis pointing left instead (as the last step before rendering we flip it back). If this camera was fixed to $\ubar{S}$, it would have the correct position and $x$-axis, but look\todo{huh? look not looks right? singular?} straight onto the seafloor. What lacks is the elevation angle $\angle(\hat{z}_S,\vec{p}_{SI})$, which we apply with $\vec{T}_{CS}\colon \udot{S}\to\udot{C}$ represented in $\ubar{C}$,
%
%This implies that $\hat{x}_C=\hat{x}_S$ and $\hat{z}_C\parallel\vec{p}_{SI}$.
%
%from the image region, but shares its orientation, $\uvec{S}=\uvec{I}$. This reduces the transformation to $\vec{T}_{SI}\colon \udot{I}\to\udot{S}$, which we represent in $\ubar{I}$ as
%
\begin{align}
\dvec{T}_{CS} &= 
\left[\begin{array}{c c}
 \mat{R}_{CS}  & \dvec{0} \\%\hline
 \dvec{0}^T  &  1
\end{array}\right] \in \mathrm{SE}(3), \label{eq_T_CS}
\end{align}\todo{comma after that equation?}
%
and define the rotation that performs the elevation as
%
\begin{align}
\dvec{R}_{CS} &= \mat{R}_x(\mathrm{atan}\frac{\dvec{p}_{SI,y}^I}{\dvec{p}_{SI,z}^I}),\label{eq_R_CS_angle}
\end{align}
%
or equivalently if $\hat{\dvec{p}}_{SI}^I$ is the normalized version of $\dvec{p}_{SI}^I$,
%
\begin{align}
\dvec{R}_{CS} &=
\left[\begin{array}{c c c}
 1 & 0                 & 0                 \\%\hline
 0 & \hat{{p}}_{SI,z}^I & -\hat{{p}}_{SI,y}^I \\
 0 & \hat{{p}}_{SI,y}^I &  \hat{{p}}_{SI,z}^I 
\end{array}\right] \in \mathrm{SO}(3). \label{eq_R_CS_coordinate}
\end{align}


\subsubsection{Radial camera coordinate conversion T$_{\mathrm{RC}}$}\label{IV_sec_radial_camera}

%\figstep{4}{\emph{Radial camera coordinate conversion.} The $\hat{x}_C$ and $\hat{y}_C$ axes span an orthogonal viewing plane onto which the scene's points can be projected. However, to simplify the boundary specification of range and elevation in the next step, we convert the coordinate representation from Cartesian to radial (cylindrical) with a non-linear function T$_{RC}\colon (\hat{x}_C,\hat{y}_C,\hat{z}_C) \mapsto (\hat{x}_R,\hat{\phi}_R,\hat{r}_R)$ subject to $\hat{x}_R=\hat{x}_C$.
%}\\[.5\baselineskip]

%T$_{RC}\colon (\hat{x}_C,\hat{y}_C,\hat{z}_C) \mapsto (\hat{x}_R,\hat{\phi}_R,\hat{r}_R)$ subject to $\hat{x}_R=\hat{x}_C$.

In a ranged imaging device only the propagation axis has Euclidean topology. The other two axes are angular: azimuth and elevation. To properly specify these boundaries (in the next step) we convert the coordinate representation from Cartesian to radial (cylindrical) with a non-linear function T$_{RC}\colon (\hat{x}_C,\hat{y}_C,\hat{z}_C) \mapsto (\hat{x}_R,\hat{\phi}_R,\hat{r}_R)$ subject to $\hat{x}_R=\hat{x}_C$,
%
\begin{align}\label{eq_T_RC}
\dvec{p}^{\k{R1}{R}}
= \mathrm{T}_{\k{R2}{R}\r{C1}{C}}(\dvec{p}^{\r{C2}{C}})
= \bmat{{p}_x^C\\
\text{atan}\big({p}_y^C / {p}_z^C\big)\\
\sqrt{({p}_y^C)^2 + ({p}_z^C)^2}
}
%
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
%  \coordinate(_MV)  at ($   (V.south)    + (-1.0\baselineskip,-1.0\baselineskip) $);
  \coordinate(_R1)  at ($ (R1.north) + ( 1.0\baselineskip,1.0\baselineskip) $);
  \path let \p1 = ($ (R2.north) + (-1.0\baselineskip,0ex) $), \p2 = (_R1) in coordinate (_R2)  at (\x1,\y2);
%   \coordinate(_I3)  at ($ (I3.north) + (-1.0\baselineskip,1.0\baselineskip) $);
  \coordinate(_R12)  at ($ .5*(_R1) + .5*(_R2) $);
  % Draw arrows and text
%           to[out=180, in=0]    (_I1)
    \draw[]                  ($ (R2.north) + (0em,\tpad) $)
           to[out= 90, in=0]    (_R12)
           to[out=180, in=90]($ (R1.north) + (0em,\tpad) $);
    }
\end{align}
%


\subsubsection{Normalization of coordinates}

% There are two main types of projection, perspective and orthographic. Perspective projection scales all $x$ and $y$ coordinates inversely with distance to create a sense of perspective in the image. Orthographic projection, in contrast, maps all the scene's vertices to a squared cube where each dimension has the range [-1,1]. This will render all objects equally large whether they are in the distance or up close. The matrix that gives orthographic projection is defined as
% %
% \begin{align}
% \left[\begin{array}{c}
% \\\dvec{p}_{XV}^{X}\\\\\hline 1
% \end{array}\right]
% = \mat{T}_{NR}
% \left[\begin{array}{c}
% \\\dvec{p}_{CV}^{C}\\\\\hline 1
% \end{array}\right]
% \end{align}
% %
% 
% \begin{align}
% \dvec{R}_{IM} &= \dvec{R}_{\mathrm{Euler}}(\psi,\theta,\phi)\label{eq_R_IM}
% \end{align}
% \begin{align}
% \dvec{T}_{NR}&\colon (x,\phi,r)\in\mathbb{R}^3 \to [r,l][b,t][n,f]
% \end{align}

OpenGL clips any coordinate that ends up outside the normalized range $[-1,1]$. After the previous step we accomplish this by linearly mapping the valid range of the along-track axis, azimuth and elevation angle into this normalized range. Such a map takes the form of an offset and normalization. each axis such that the pixels we want visible end up in this range.
Defining $\udot{I}_{\hat{x}\hat{y}}$ as the point $I(\min,\min,0)$ and $\udot{I}_{\check{x}\check{y}}$ as the point $I(\max,\max,0)$.\todo{Not sold on this syntax...}

\todo{Add references to the cross and check graphics throughout}

Let ${\dvec{s}_\Delta^I}$ be a tuple of positive scalars that defines the extent of each axis in the image:\todo{Keep position vector bold, even if an element is indexed?}
%$\dvec{p}_\Delta = \udot{I}_{
%
%\dvec{p}_{I_\Delta} = \big[p_{\Delta,i}\big] &= \big[|p_{I_\mathrm{max},i}-p_{I_\mathrm{min},i}|\big] \quad i\in\{x,y,z\}
\begin{align}\label{eq:M}
\dvec{s}_\Delta^I &= \mathrm{abs}\big(\dvec{p}_{I_\mathrm{max}}^I-\dvec{p}_{I_\mathrm{min}}^I\big),
\end{align}
%
where abs($\cdot$) denotes element-wise absolute value. Then we obtain $\dvec{s}_\Delta^C$
\begin{align}\label{eq:M}
\dvec{s}_\Delta^C &= \mathrm{T_{RC}}\big(\mat{T}_{CS}\mat{T}_{SI}\dvec{s}_\Delta^I\big).
\end{align}
%
Now we have all the information needed to normalize the scene in $\ubar{C}$, i.e. the contents of $\mat{T}_{NR}$:
%
\begin{align}
\mat{L}_{NR}
&= \diag\left(\boldsymbol{2}\oslash\dvec{s}_\Delta^C\right) \label{eq_L_NR} \\
\dvec{t}_{NR}
&= -2\dvec{p}_{CI}^C\oslash\dvec{s}_\Delta^C \label{eq_t_NR}
\end{align}
%
where $\boldsymbol{2}$ is a vector of twos\todo{Too informal?}, $\oslash$ denotes Hadamard (element-wise) division, and diag($\cdot$) constructs a diagonal matrix from its input vector. 



\subsubsection{OpenGL rendering}

To produce the sonar templates we assume a rough, isotropic surface that reflects energy equally in all directions. This permits use of a Lambertian scattering model where the backscatter intensity depends only on the incidence angle~\cite{Zhang1999}. It does not consider observation angle or sound frequency, but for the purpose of creating templates this is not needed.   

The rendered image will appear as if we placed a window at the sonar and looked through it in the direction of the image. This window is resized to make sure that the template image perfectly fills it.

\todo{
- Facets
}

\subsubsection{OpenCL post-processing}

The ``camera image'' rendered with OpenGL is not in along-track and cross-track coordinates as we want it to be. It does, however, show us the parts of the scene visible from the sonar. OpenGL can also produce a depth map that reveals the distance from the camera to to each image pixel. This information can be converted to a ranged sonar-like image by adding up all the intensity values that share the same depth for each range line. We perform this computation in OpenCL. It allows general purpose programming on GPUs and can interoperate with OpenGL quite nicely. This way we keep all the calculations on the GPU.

\begin{align}
&\text{forall } y,\varphi \nn
&\quad\text{if }\left\lfloor y_I\right\rfloor == y \nn
&\qquad\dvec{a}_I[y] \colon= \big(y_I - \left\lfloor y_I\right\rfloor\big)\dvec{a}^C[\varphi] \nn
&\qquad\dvec{a}_I[y+1] \colon= \big(1 - y_I - \left\lfloor y_I\right\rfloor\big)\dvec{a}^C[\varphi]
\end{align}
%y_I &= \mat{T}_{IC}\dvec{d}_C[\varphi] \\
%\dvec{a}_I[y] &= \suml{\forall \varphi} \begin{cases}
%\big(y_I - \left\lfloor y_I\right\rfloor\big)\dvec{a}^C[\varphi] & \text{if}\quad \left\lfloor y_I+0.5\right\rfloor == y, \\
%\big(y_I - \left\lfloor y_I\right\rfloor\big)\dvec{a}^C[\varphi] & \text{if}\quad \left\lfloor y_I\right\rfloor == y, \\
%0            & \text{otherwise}
%\end{cases}

\todo{
- Kernel tuning.
- Sharing. PBO. Asynchronous.
- Write interpolation
}

\subsection{Speed considerations}

Using OpenGL and OpenCL to generate sonar templates on the GPU allow hundreds of sonar templates to be formed per second. While this is sufficient for our needs.\todo{Hanging. Weak.} However, initializing OpenGL and OpenCL and loading 3D models from file into it is a slow process, and typically takes several seconds for every invocation. Therefore, to alleviate the simulation performance we either had to supply it with a list of parameters or ensure that it did not have to be shut down for every invocation. 

%The rendering loop that uses OpenGL and OpenCL to form 

\todo{
Rewrite
Write to file
}

%    """
%    
%    stm: Standard template matching, made with SIGMAS+.
%    atm: Adaptive template matching, made with FFISim.
%    atm_bury: Adaptive template matching where a search of bury depths is used.
%    atm_bury_restr: Restricted?
%    
%    Adaptiveness made by letting the template matcher use 5 different rotation
%    angles in the ground-range axis. 
%    
%    
%    Data from Jesusbukta
%    
%    Figures are from run 2012.06.16
%    det210 - Cylinder 1: est 2.92m (short side towards sonar)
%    det233 - Cylinder 2: est 3.05m (diagonal)
%    det379 - Torpedo: est 5.06m (looks good)
%    
%    ROC curves from 3 different runs, first two just 2 days apart in 2009, last one in 2012.
%    
%    run_20120616 - last one. Much better with adaptive templates.
%    
%    ROC curves made by average correlation score of shadow/highlight from template matching.
%    
%    
%    fig4b(2015.11.16)
%       Bug that skipped masking of port side (negative y) detections past 180m range fixed
%       
%    fig4 (2015.11.16)
%       Bug leading to incorrect estimation of cylinder orientation fixed.
%          This affects run090601_1. Should be better after this.
%          
%    fig3 (2015.11.09)
%       Detections outside 180m range have been masked (but bug for port side fixed in fig4b).
%       Fixed an incorrectly annotated detection.
%       
%    fig2 (2015.11.02)
%       Height of object estimated from shadow length
%       Let adaptive method try 5 different rotations in ground-range (-4, -2, 0, 2, 4)
%          Improved results, but incorrect classification of:
%          5 detections in first run, and
%          3 detections in second run
%          => For FPR=0.2 the results are slightly worse than with normal adaptive (particularly first run).
%          Images are problematic in these cases:
%             2 detections get wrong orientation because a line (anchor) cross the image
%             Other misclassifications in image regions with poor echo or shadow strength
%          => Not a big issue as not enough time to detect more than ~20% of the detections in a run
%             In a MCM operation these detections would never be used.
%             The runs have 3000 detections each, photographing 20% of these is too much.
%          Performance FPR>0.2 not interesting.
%          Performance FPR<0.2 much better with adaptive template matching.
%          
%    fig (2015.10.23)
%       atm - cylinder orientation and length estimated from SAS image and fed into simulator.
%       
%       ROC curves from last run. Computing the mean correlation score of the shadow and highlight/echo.
%       Early results. Can only check vs. cylinder-like objects.
%       Standard template library created based on estimation of physical size of known cylinder mines.
%       Not possible for adaptive method to perform better when hitting such a target.
%       
%  
%    """
\newlength\imgspacing\setlength\imgspacing{.5cm}
\setcounter{topnumber}{1}
\setcounter{totalnumber}{1}

\section{Results \& Discussion}

\begin{figure*}[t]\centering%
\ifOverLeaf%
  \includegraphics[width=.8\linewidth]{gfx/fig_images_sonar_simulator_tagged.pdf}%
\else%
  \includegraphics[width=.8\linewidth]{gfx/fig_images_sonar_simulator_tagged.svg}%
\fi
\caption{\emph{Two cylinders and a torpedo in Jesus Bay, Norway.} Top row shows isolated SAS image segment corresponding to the object, while center and bottom row show the template from the conventional and adaptive method, respectively. The contour of the SAS object is indicated with red lines. Observe that the adaptive technique perform better in the highlight, but not necessarily the shadow.}\label{IV_fig_images_sonar_simulation}%
\end{figure*}

\begin{figure*}[t]\centering%
\includegraphics[width=.8\linewidth]{gfx/fig_overlay.pdf}%
% \parbox{\linewidth}{\small\centering\vspace{.5\baselineskip}
% \newline\centering\newline %\\[.5\baselineskip]
\newcommand\cdesc[2]{{\raggedright\setlength\fboxsep{0pt}
\fbox{\colorbox[HTML]{#1}{\vrule height8.5pt depth3.5pt width0pt\hspace{.5cm}}}\ \ #2\\}}
\begin{minipage}{.29\linewidth}\footnotesize
\cdesc{800000}{Template highlight and image highlight}
\cdesc{FF1000}{Template highlight only}
\cdesc{FFEB00}{Image highlight only}
\cdesc{83FF7C}{Background pixels}
\end{minipage}\mbox{}\hspace{3cm}
\begin{minipage}{.29\linewidth}\footnotesize
\cdesc{000083}{Template shadow and image shadow}
\cdesc{0014FF}{Template shadow only}
\cdesc{00EFFF}{Image shadow only}
\cdesc{00A7FF}{Template shadow and image highlight}
\end{minipage}%}
\caption{Comparison of classification performance for the standard and adaptive template technique. The three objects were more or less arbitrary picked from the scene. For the roughly 3\,m long cylinders both methods perform similarly, the standard method seemingly with a slight edge. This is no surprise given that the 3\,m long cylinder model that make up the static template database fit the actual object well here. However, for the longer torpedo-like object the adaptive technique is clearly better. This is its merit; for objects and geometries that are not included in the static template database we can expect the adaptive techniques to perform better.}\label{IV_fig_image_simulation}%
\end{figure*}

\begin{figure*}[t]\centering%
\includegraphics[width=\linewidth]{gfx/fig_rocs.pdf}%
\caption{\emph{Receiver operating characteristics (ROC) curves.} The adaptive methods }\label{IV_fig_roc_curves}%
\end{figure*}

The upcoming results will be based on experimental data from a HISAS1030 interferometric synthetic aperture sonar (SAS) attached to a HUGIN autonomous underwater vehicle (AUV), both developed by Kongsberg Maritime, Norway. The HISAS1030 is a fully digital phased array with 32 hydrophones, 100\,kHz center frequency and 30\,kHz bandwidth. It is 1.2\,m long, has a half-power beamwidth (HPBW) of 23$^\circ$, and is capable of generating SAS images with a theoretical resolution of 3-4\,cm both cross- and along-track. Our version of HUGIN also carries a camera that is used for optical inspection of interesting objects.

%To test performance of our simulator and template matching techniques where measured using experimental data from 

We present three different HUGIN runs from Jesus Bay in Norway. The first two a couple days apart in January 2009, containing roughly 2700 and 3200 detections, respectively. The last is from June 2012 and contains roughly 650 detections. Almost all objects here are cylindric---e.g. barrels, pipe mines and torpedoes---with a length of about 3\,m.

The following three methods will be contrasted:\todo{Poor transition}
%
\begin{itemize}
\item \emph{Standard (SIGMAS+).}\todo{Inconsistent list formatting and style} Picks the best template out of a set precomputed with SIGMAS+. The templates are tailored to a 3\,m long cylinder; the most frequently occurring object in the scene.
\item \emph{Adaptive, normal (FFISim).} Creates a single template with FFISim, given estimated seafloor and object orientation, and object length, width and immersion depth~\cite{Midelfart2010}. 
\item \emph{Adaptive, buried (FFISim).} Finds an "optimal" template with FFISim, given the estimated parameters described above and a search space for burial depth---modeled as rotation by $\{-4,-2,0,2,4\}^\circ$ around the cross track axis $y_\textrm{\tiny$\uvec{I}$}$.
\end{itemize}

\subsection{Processing speed}

%FFISim was designed to 
%The key selling point of FFISim is that it is very fast. 
%Note that FFISim 
%
%is used with the adaptive method due to its tighter integration with the classifier, allowing a range of templates to be generated and assessed on the GPU in a single invocation. It can produce thousands of templates per second if need be. This is why FFISim is being used in the adaptive procedure, and SIGMAS+ in the conventional procedure with a static precomputed set of templates.


Our GPU-based simulator is high-speed. On a computer with a six-core Intel Core i7-3930K and a Radeon HD7970 it computes almost 1000 templates per second, each being 1 megapixel large. This makes it possible to tailor the templates to very closely imaged objects.\todo{Huh?} By contrast, in the standard approach a template library must be created beforehand from a limited set of parameter values. Therefore, it is possible no template in the library closely fits the object in the image even though the object is a target of interest.


% Hence, it may happen that no template in the library is a close fit to the object in the image even though the object is actually a target of interest.\todo{Rewrite paragraph}

%The edge of FFISim lies in its tighter integration with the classifier


\subsection{Classification - selected objects}


Let us first study simulation and classification performance on three typical objects from the 2012 data: two roughly 3\,m long mines and a 5\,m long torpedo, each with unique immersion depth and aspect angle. We present the SAS image segments and templates for these objects in \Fig{IV_fig_images_sonar_simulation}, with hand-drawn red lines superimposed to mark the objects' contours. Observe the following:
%
\begin{itemize}
\item \emph{Template image quality} is similar for both SIGMAS+ and FFISim. With accurate input parameters, both simulators generate highlight and shadow regions with correct size and shape. Intensity values are also nearly identical (ignoring background level), as expected since both rely on a Lambertian scattering model.
\item \emph{Template accuracy} improves when using the adaptive technique; it excels at generating accurate highlight regions, but is less convincing in shadow regions. The less common the object, the poorer its representation by the static database, the greater the advantage of the adaptive method.\todo{Is this better? Not entirely happy still...}
\end{itemize}



\subsection{Classification - all objects}

We demonstrate classification performance with receiver operating characteristic (ROC) curves in \Fig{IV_fig_roc_curves}. 

Below 20\% false positive rate (FPR) the adaptive techniques outperformed the static method in every run. This benefits e.g. mine counter measures (MCM) operations because out of 3000 detections only a few will be visually inspected up close.

Above 20\% FPR the results are less clear: the adaptive techniques performed similarly to the static method in 2012, but worse January 6. and better January 8. 2009---with 5 and 3 false detections at 20\% FPR, respectively. From visual inspection we found the cause to be a combination of fish trawler tracks and poor highlight and shadow quality.



\section{Conclusion}\label{IV_conclusion}

\todo{Start with the key selling points first. Inference, not deference}

One way to automatically classify objects in SAS images is to compare the imaged objects with a predefined set of templates. However, this is suboptimal as it is infeasible to create accurate templates for all the relevant configurations of seabeds and objects. To solve this we have implemented a SAS simulator that creates the templates in delayed real-time based on parameters estimated from the current scene. In our studies this improves the match between the SAS objects and their corresponding template significantly in most cases.

To obtain the delayed real-time performance we implemented the simulator on a GPU. These devices have a theoretical peak performance that is typically an order of magnitude higher than CPUs in a comparable price range. We found this potential to be effectively utilized with the well matured and highly optimized OpenGL graphics processing API. Our simulator uses this framework for most of the scene processing.

A final post-processing step is performed in OpenCL, which allows general-purpose GPU programming. It interoperates well with OpenGL and improves simulation flexibility. This is valuable as the simulator is still being actively developed and will likely see new features that can not be easily implemented in OpenGL alone.

With good image quality or low FRP, or both, the adaptive methods did better. However, when this was not the case, it was less clear who the winner was. The standard method of using a static template library generally did well here.\todo{Meh. Fix fix fix}

In terms of template quality both SIGMAS+ and FFISim are similar. The main difference lies in how they integrate into the classification process: while SIGMAS+ generates one template for each invocation, FFISim can iterate through a space of parameters on the GPU before returning control to the CPU. This speeds up the template matching dramatically, allowing it to assess thousands of templates per second instead of, say, one. 
 
%
%\begin{itemize}
%\item Motion compensation
%\item 
%\end{itemize}

\ifPhdDoc
\clearpage
\appendix
\renewcommand\thesection{\Roman{section}}
\else
\appendices
\fi

\section{Active rotations and transforms}\label{IV_active_rotations_transforms} 

The geometrical intent of an active rotation is to move a point, as opposed to the passive version which instead changes the point's representation. With our notation this can be expressed as either
%
\begin{align}
\m{CL}
\vec{p}_{M\g{Vtot}{V}_{\color{Green}\!\textrm{new}}}
= \vec{R} \; \vec{p}_{M\g{V2}{V}}\m{CR}
\quad\text{or}\quad
\m{CL2}
\dvec{p}_{M\g{Vtot2}{V}_{\color{Green}\!\textrm{new}}}^M
= \mat{R}^M \dvec{p}_{M\g{V4}{V}}^M,\m{CR2}
\label{IV_eq_active_rotation}
 \\\nonumber
 %\qquad\qquad
%
%\\[.5\baselineskip]\nonumber % Adjust spacing as necessary
\tikz[overlay,remember picture]{
  % Set up some general bounding box markers
  \coordinate(CC) at      ($ .5*(CL)       + .5*(CR)                         $);
  \coordinate(CC2) at     ($ .5*(CL2)      + .5*(CR2)                        $);
  \coordinate(_MV) at     ($ (V2.south)    + (-\baselineskip,-\baselineskip) $);
  \coordinate(_MVtot) at  ($ (Vtot.south)  + ( \baselineskip,-\baselineskip) $);
  \coordinate(_MV2) at    ($ (V4.south)    + (-\baselineskip,-\baselineskip) $);
  \coordinate(_MVtot2) at ($ (Vtot2.south) + ( \baselineskip,-\baselineskip) $);
  % Draw arrows and text
    \draw[]                    ($ (V2.south)    - (0em,\tpad) $)
            to[out=-90, in=0]     (_MV)
            to[out=180, in=0]     (_MVtot)
            to[out=180, in=-90]($ (Vtot.south)  - (0em,\tpad) $);
    \draw[]                    ($ (V4.south)    - (0em,\tpad) $) 
            to[out=-90, in=0]     (_MV2)
            to[out=180, in=0]     (_MVtot2)
            to[out=180, in=-90]($ (Vtot2.south) - (0em,\tpad) $);
}
\end{align}
%
%
% Implicit representation: (x,y,z) subject to x²+y²+z²=c -> no 
depending on whether the free or decomposed form is sought, respectively, and where $\vec{R}\colon \mathbb{V}^N \to \mathbb{V}^N$ and $\dvec{R}^M\colon \mathbb{R}^N \to \mathbb{R}^N$. Beware that the rotation matrix can be identical---both notationally and numerically---in the active and passive case, for example, if $\dvec{R}^M$ was replaced by $\dvec{R}_{IM}$ in \eq{IV_eq_active_rotation}. Thus, the geometrical intent must be inferred from the position vector notation; either the point changes, or the representation does, but never both.



\section{Transformation matrices}\label{IV_transformation_matrices} 

and their orientations by a rotation dyadic $\vec{R}_{IM}$ mapping any vector in $\uvec{M}$ to $\uvec{I}$ (which includes the basis vectors),
%
\begin{align}
\vec{R}_{IM}\colon \uvec{M} &\to \uvec{I}\label{eq_rotation_dyadic}\\
\vec{M}_i &\mapsto \vec{I}_i \quad \forall i. \nonumber
\end{align}

We define $\vec{R}_{AB}$ as a rotation---from $\uvec{A}$ to $\uvec{B}$---by angle $\theta\in\mathbb{R}$ clockwise around unit vector $\hat{r}\in\mathbb{V}$ in $\mathbb{R}^3$ Euclidean space using Euler-Rodrigues' formula,

\subsection{Free Euler-Rodrigues}\label{IV_sec:free_euler_rodrigues}
%
\begin{align}\label{eq:M}
%\uvec{A} = \vec{R}_{\uvec{A}\uvec{B}}\uvec{B} \\
\vec{R}_{AB}
&\triangleq %e^{j\overset{\times}{r}\theta}=\nn
 \vec{I} + \overset{\times}{r}\sin\theta + \overset{\times}{r}{}^2(1-\cos\theta) \in SO(3),
\end{align}
%
where $\vec{I}$ is the identity, and $\overset{\times}{r}$ is the skew-symmetric (or cross-product) matrix of $\hat{r}$,
%
\begin{align}
\overset{\times}{r}
&\triangleq \bmat{\hat{r}\times} \triangleq \bmat{
0 & -\hat{r}_z & \hat{r}_y \\
\hat{r}_z & 0 & -\hat{r}_x \\
-\hat{r}_y & \hat{r}_x & 0
},
\end{align}
%
and the angle $\theta$ and rotation axis $\hat{r}_{AB}$ can be expressed in terms of $\vec{p}_A$ and $\vec{p}_{B}$,
%
\begin{align}
\theta
&= \mathrm{acos}\frac{\vec{p}_{A}\cdot\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert} \\
\hat{r}_{AB} &=  \frac{\vec{p}_{A}\times\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert}
\end{align}
%
where $\cdot$ and $\times$ denotes the dot- and cross-product, respectively.\todo{Mighty long and heavy sentence... Yikes}

$SO(3)$ refers to special (\emph{i.e.} right handed) orthogonal $\mathbb{R}^{3\times3}$ matrices, where the handedness is verified by determinant $\big|R\big|=1$ (as opposed to $\pm1$), and the orthogonality follows from the skew symmetry of $\overset{\times}{r}_{AB}$. $SO(3)$ are also known as rigid transformations, because they preserve the distance and orientation between points~\cite{Murray1994}\footnote{Pages 26-27}\todo{Valid page specification?}.

{\color{blue}Below is the old text}

We restrict our attention to linear transforms of the kind $\mat{L}_{AB} = \mat{R}_{AB}\;\mat{S}_{AB}$ where the former is a rotation matrix and the latter a scaling matrix. The rotations are performed with Euler-Rodrigues formula~\cite{Dai2015} using a unit length rotation axis $\dvec{\hat{r}}_{AB}^A$ and angle $\theta_{AB}$:
%
\begin{align}\label{eq:M}
\mat{R}_{AB} &= \I + \sin\theta_{AB}\;[\dvec{\hat{r}}^A\times] + (1-\cos\theta_{AB})\;[\dvec{\hat{r}}^A\times]^2 \in\mathbb{R}^{3\times3}
\end{align}
%
where $\big[\dvec{\hat{r}}_{AB}^A\big]$ is the open-right cross product matrix of $\dvec{\hat{r}}^A$ and $\mat{I}$ is the identity matrix. 

% %\dvec{\hat{p}}_{A\times{}B}^C
\begin{align}
\theta_{AB}
&= \mathrm{acos}\frac{\dvec{p}_{A}^{C}\cdot\dvec{p}_{B}^{C}}{\lVert\dvec{p}_{A}^{C}\rVert\lVert\dvec{p}_{A}^{C}\rVert} \\
\dvec{\hat{r}}_{AB}^C &=  \frac{\dvec{p}_{A}^{C}\times\dvec{p}_{B}^{C}}{\lVert\dvec{p}_{A}^{C}\rVert\lVert\dvec{p}_{A}^{C}\rVert}
\end{align}

% http://lairs.eng.buffalo.edu/pdffiles/pconf/C10.pdf
% http://www.navlab.net/Publications/Inertial_Navigation_-_Theory_and_Applications.pdf

Each data model that we load into OpenGL is defined in its own local coordinate system. To map them into OpenGL world coordinates a set of common transformation matrices are applied to each model's vertices:
%
\begin{align}
\bmat{x_t \\ y_t \\ z_t \\ w}_\text{\parbox{1cm}{\setlength\baselineskip{0.3cm}transformed\\model}} &= \boldsymbol{T} \cdot \boldsymbol{R} \cdot \boldsymbol{S} \cdot \bmat{x \\ y \\ z \\ w}_\text{model}.
\end{align}
%
Here a model vertex at position $(x,y,z)$ is first scaled with $\boldsymbol{S}$, then rotated with $\boldsymbol{R}$ and finally translated with $\boldsymbol{T}$, resulting in the transformed coordinates $(x_t,y_t,z_t)$. The parameter $w$ defines whether the vertex is a position ($w=1$) or a direction ($w=0$), which need to be handled differently. While these matrices are well-known in the field of computer graphics, there exist several different conventions but we supply them here for completeness.

%Adding some phantom space to align the columns of the next matrices
\newcommand\pc[1]{\phantom{ml}#1\phantom{ml}}
The scaling and translation matrices are simple:
%
\begin{align}
\boldsymbol{S}(\s) &= \bmat{
   s_x  &  0   & 0  & 0 \\
   0    &  s_y  &  0    &  0 \\
   0    &  0    &  s_z  &  0 \\
   \pc{0}    &  \pc{0}    &  \pc{0}    & \pc{1} \\
  },
  \end{align}
%
and
%
\begin{align}
\boldsymbol{T(\vec\Delta}) &= \bmat{
	1  &  0  &  0  &  \Delta_x \\
	0  &  1  &  0  &  \Delta_y \\
	0  &  0  &  1  &  \Delta_z \\
 \pc{0}  &   \pc{0}  &   \pc{0}  &   \pc{1} \\
},
\end{align}
%
where $\s = (s_x, s_y, s_z)$ and $\vec\Delta = (\Delta_x, \Delta_y, \Delta_z)$ are the scaling and translation factors.

Rotations will be specified using yaw ($\phi$), pitch ($\theta$) and roll ($\psi$), representing clockwise rotations around the intrinsic $z$-, $y$- and $x$-axis, respectively. Rotation order follow the intrinsic Tait-Brian $Z$-$Y'$-$X''$ convention, in which roll is applied first, then pitch and finally yaw:
%
\begin{align}
\R(\phi_x) = \R_z(\phi)\R_y(\theta)\R_x(\psi),
\end{align}
%
where
%
\begin{align}
\R_x(\psi) &= \bmat{
    1  &   0        &  0         &  0 \\
    0  &  \cos\psi  &  -\sin\psi &  0 \\
    0  &  \sin\psi  &  \cos\psi  &  0 \\
\pc{0} &  \pc{0}    &  \pc{0}    &  \pc{1} \\
}, \\[.5\baselineskip]
\R_y(\theta) & = \bmat{
\cos\theta	& 0          & \sin\theta & 0 \\
0				& 1          & 0          & 0  \\
-\sin\theta & 0          & \cos\theta & 0  \\
\pc{0}      &  \pc{0}    & \pc{0}     & \pc{1} \\
},
%\\[0\baselineskip]
\intertext{and}
%\\[0\baselineskip]
\R_z(\phi) &= \bmat{
\cos\phi	&  -\sin\phi   &  0      &  0 \\
\sin\phi	&  \cos\phi    &  0      &  0 \\
0			&  0           &  1      &  0 \\
\pc{0}	&  \pc{0}      &  \pc{0} &  \pc{1} \\
}.
\end{align}
%
Note that at $\theta=\frac{\pi}{2}$ we have $\phi=-\psi$, and at $\theta=-\frac{\pi}{2}$ we have $\phi=\psi$. This effect, in which one degree of freedom is lost, is known as Gimbal lock. Any three-parameter specification of an orientation in three-dimensional space has this problem. Adding a fourth parameter can resolve the issue. e.g. by direct specification of orientation in terms of a rotation axis and angle. This can be achieved using the Euler-Rodrigues formula or with quaternions. The latter has the added benefit of reduced computational complexity, improved numerical stability and simple ways to perform spherical interpolations. An alternative used extensively at FFI that is non-singular and practical is the $\vec n$-vector notation~\cite{Gade2010}. However, we stick with the Euler-angle representation here due to their simplicity.


%xyz - system space coordinates (fixed unmoving)
%XYZ - system body coordinates (body fixed moving)


Translation is performed last. 

%and the bases in terms of a linear operator $\vec{R}_{AB} \in SO(3)$ which rotates about an axis $\hat{r}$ by angle $\theta$,
%%
%\begin{align}\label{eq:M}
%\vec{R}_{AB}
%&=
%% \vec{I} + \overset{\times}{r}\sin\theta + \overset{\times}{r}{}^2(1-\cos\theta).
%\vec{I} + \sin\theta(\hat{r}\times) + (1-\cos\theta)(\hat{r}\times)^2.
%\end{align}
%%
%where $\times$ is the cross-product. $\hat{r}$ and $\theta$ relates to $\uvec{A}$ and $\uvec{B}$ by
%%
%\begin{align}
%\hat{r} = \frac{\uvec{A}\times\uvec{B}}{\lVert\uvec{A}\rVert\lVert\uvec{B}\rVert}
%\quad\mathrm{and}\quad
%\theta  = \mathrm{acos}\frac{\uvec{A}\cdot\uvec{B}}{\lVert\uvec{A}\rVert\lVert\uvec{B}\rVert},
%\end{align}
%%
%where $\times$ and $\cdot$ signifies the cross- and dot-product, respectively.
%
%and the angle $\theta$ and rotation axis $\hat{r}_{AB}$ can be expressed in terms of $\vec{p}_A$ and $\vec{p}_{B}$,
%%
%\begin{align}
%\theta
%&= \mathrm{acos}\frac{\vec{p}_{A}\cdot\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert} \\
%\hat{r}_{AB} &=  \frac{\vec{p}_{A}\times\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert}
%\end{align}
%\begin{align}
%\theta
%&= \mathrm{acos}\frac{\vec{p}_{A}\cdot\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert} \\
%\hat{r}_{AB} &=  \frac{\vec{p}_{A}\times\vec{p}_{B}}{\lVert\vec{p}_{A}\rVert\lVert\vec{p}_{B}\rVert}
%\end{align}
%%
%The position vector $\vec{p}_{\udot{A}\udot{B}}$ go from $\udot{A}$ to $\udot{B}$ only depends on these points. Similarly, the matrix $\vec{R}_{\uvec{A}\uvec{B}}$ only depends on vectors; $\vec{R}_{\uvec{A}\uvec{B}}\uvec{B}$ rotates $\uvec{B}$ to $\uvec{B}$, $\uvec{A}\vec{R}_{\uvec{A}\uvec{B}}$ rotates $\uvec{A}$ to $\uvec{B}$. 
%


% use section* for acknowledgement
\ifCLASSOPTIONcompsoc% % This command fixes abstract positioning for compsoc articles:
% \IEEEdisplaynotcompsoctitleabstractindextext
% 
% % (Optional) Add some extra info on cover page of peer review papers:
% % \ifCLASSOPTIONpeerreview
% % \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% % \fi
% 
% % Insert page break and insert second title (peer review mode)
% \IEEEpeerreviewmaketitle
% 
% 
% 
  \section*{Acknowledgments}
\else
  \section*{Acknowledgment}
\fi


The authors would like to express their gratitude to the Norwegian Defence Research Establishment (FFI) for funding the development of the simulator.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\ifPhdDoc
%   \printbibliography[title=References,heading=subbibliography]
%    \bibliographysty
%    \bibliography{library.bib}
%    \print
\else
   \ifBuildBibliography
      \typeout{===Mybib===}
      \bibliographystyle{IEEEtran}
      \bibliography{references}

   \else
      \typeout{===MyStaticbib===}
      % Paste here
   \fi

  
   % Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
   % \begin{thebibliography}{10}
   % stuff here
   % \end{thebibliography}
   
   
   \input{bio/jo_inge_buskenes_new}
   % \input{bio/jon_petter_aasen}
   % \input{bio/carl-inge_nilsen}
   % \input{bio/andreas_austeng}
   
   
   % 
   % IMAGE METRICS
   %
   % - Point spread function (res via main lobe width, SAS gain via PDF height)
   % - Constrast measures
   %
   % ACR = cL/2
   % C = (s+n)/n ratio
   % - 
   % WHO's needing processing power
   % - Centre for Maritime Research and Experimentation
   
   
   \vfill 
   
   \input{derivations}

\fi




%\begin{align}\label{eq:M}\setlength{\extrarowheight}{-1.5ex}%\renewcommand{\arraystretch}{1.5}
%\mat{T}_{IM}
%= \left[
%\begin{tabularx}{.8\linewidth}{*{3}{>{\centering\arraybackslash}X} | c}
%	\multicolumn{3}{c|}{
%   $\diag\left(\dfrac{2}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}\right)$}
%   & 
%   $\dfrac{-2\dvec{p}_{OI}^O}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}$
%    \\\multicolumn{3}{c|}{}\\\hline\multicolumn{3}{c|}{}\\
%   0 & 0 & 0  & 1
%\end{tabularx}\right]
%\end{align}
%%\multicolumn{1}{|c}{\multirow{1}{*}{
%
%%\begin{align}\label{eq:M}
%%\mat{T}_{IM}
%%= \left[\begin{array}{c c c | c}
%%& & &   \\
%%& \mat{R}_{IM}(\psi,\theta+\pi,\phi)\;\mat{S}_{IM} & & \dvec{p}_{IM}^{I} \\
%%& & &  \\\hline
%% 0 &  0  &  0  &  1
%%\end{array}\right]
%%\end{align}
%
%\begin{align}\label{eq:M}
%\mat{T}_{XO}
%= \left[\begin{array}{c c c | c}
%%& & & \\
%\multicolumn{3}{l|}{\diag\left(\dfrac{2}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}\right)} &
%%&&&
%\dfrac{-2\dvec{p}_{OI}^O}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|} \\
%%& & & \\
%%& & & \\
%%& & & \\
%\multicolumn{3}{l|}{0 \hfill 0 \hfill 0} & 1
%%0 & 0 & 0 & 1
%\end{array}\right]
%\end{align}



%
%\begin{table}[h]\centering
%\begin{center}
%\begin{tabular}{l r r r}
%	                   & $\mat{T}_{CS}$                                                                     & $\mat{T}_{SI}$      & $\mat{T}_{IM}$      \\ \hline
%	$\theta$           & acos$\frac{\uvec{I}_{\,z}^S\cdot\dvec{p}_{SI}^{S}}{\lVert\dvec{p}_{SI}^{S}\rVert}$ & 0                   & $\psi$              \\
%	$\dvec{\hat{r}}\T$ & $\uvec{I}_{\,z}^S \times \dvec{p}_{SI}^{S}$                                        &                     & $\big(0,1,0\big)$   \\
%	$\Phi_y$           & 0                                                                                  & 0                   & $\pi+\theta$        \\
%	$\Phi_y$           & 0                                                                                  & 0                   & $\pi+\theta$        \\
%	$\Phi_z$           & 0                                                                                  & 0                   & $\phi$              \\
%	$\mat{L}$          & $\mat{R}(\uvec{I}_{\,z}^S, \dvec{p}_{SI}^{S})$                                     & 0                   & $\mat{R}(\psi,\theta,\phi)\mat{S}$              \\
%   $\mat{S}$
%	$\s_x$             & 1                                                                                  & 1                   & 1                   \\
%	$\s_y$             & 1                                                                                  & 1                   & 1                   \\
%	$\s_z$             & 1                                                                                  & 1                   & 1                   \\
%	$\vec{t}$          & $\dvec{p}_{CS}^{O}$                                                                & $\dvec{p}_{SI}^{S}$ & $\dvec{p}_{IM}^{I}$
%\end{tabular}
%%\end{table}
%\end{center}

%\begin{table}[h]\centering
%\begin{center}
%\begin{tabular}{l r r r}
%	          & $\mat{T}_{CS}$                                                                          & $\mat{T}_{SI}$      & $\mat{T}_{IM}$      \\ \hline
%	$\theta$  & $\frac{\pi}{2}-\text{atan}\left(\frac{\dvec{p}_{SI,z}^{S}}{\dvec{p}_{SI,y}^{S}}\right)$ & 0                   & $\psi$              \\
%	$\Phi_y$  & 0                                                                                       & 0                   & $\pi+\theta$        \\
%	$\Phi_y$  & 0                                                                                       & 0                   & $\pi+\theta$        \\
%	$\Phi_z$  & 0                                                                                       & 0                   & $\phi$              \\
%	$\s_x$    & 1                                                                                       & 1                   & 1                   \\
%	$\s_y$    & 1                                                                                       & 1                   & 1                   \\
%	$\s_z$    & 1                                                                                       & 1                   & 1                   \\
%	$\vec{p}$ & $\dvec{p}_{CS}^{O}$                                                                     & $\dvec{p}_{SI}^{S}$ & $\dvec{p}_{IM}^{I}$
%\end{tabular}
%%\end{table}
%\end{center}

%\begin{align}
%\theta_{CS}
%\dvec{\hat{r}} = \frac{\dvec{\hat{n}}_I\times\dvec{p}_{SI,z}^{S}}{\lVert\dvec{p}_{SI,z}^{S}\rVert})
%\end{align}
%$\frac{\pi}{2}-\text{atan}\left(\frac{\dvec{p}_{SI,z}^{S}}{\dvec{p}_{SI,y}^{S}}\right)$
%



%\begin{align}\label{eq:M}\setlength{\extrarowheight}{-1.5ex}%\renewcommand{\arraystretch}{1.5}
%\mat{T}_{IM}
%= \left[
%\begin{tabularx}{.8\linewidth}{*{3}{>{\centering\arraybackslash}X} | c}
%	\multicolumn{3}{c|}{
%   $\diag\left(\dfrac{2}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}\right)$}
%   & 
%   $\dfrac{-2\dvec{p}_{OI}^O}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}$
%    \\\multicolumn{3}{c|}{}\\\hline\multicolumn{3}{c|}{}\\
%   0 & 0 & 0  & 1
%\end{tabularx}\right]
%\end{align}
%\multicolumn{1}{|c}{\multirow{1}{*}{

%\begin{align}\label{eq:M}
%\mat{T}_{IM}
%= \left[\begin{array}{c c c | c}
%& & &   \\
%& \mat{R}_{IM}(\psi,\theta+\pi,\phi)\;\mat{S}_{IM} & & \dvec{p}_{IM}^{I} \\
%& & &  \\\hline
% 0 &  0  &  0  &  1
%\end{array}\right]
%\end{align}

%\begin{align}\label{eq:M}
%\mat{T}_{XO}
%= \left[\begin{array}{c c c | c}
%%& & & \\
%\multicolumn{3}{l|}{\diag\left(\dfrac{2}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}\right)} &
%%&&&
%\dfrac{-2\dvec{p}_{OI}^O}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|} \\
%%& & & \\
%%& & & \\
%%& & & \\
%\multicolumn{3}{l|}{0 \hfill 0 \hfill 0} & 1
%%0 & 0 & 0 & 1
%\end{array}\right]
%\end{align}
%%&                                                                                                          \\
%	  & \diag\left(\dfrac{2}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|}\right) &   & \dfrac{-2\dvec{p}_{OI}^O}{|\dvec{p}_{OI_{\check{x}\check{y}}}-\dvec{p}_{OI_{\hat{x}\hat{y}}}|} \\
%	  &                                                                                                            &   &                                                                                                          \\ \hline
%	0 & 0                                                                                                          & 0 & 1
